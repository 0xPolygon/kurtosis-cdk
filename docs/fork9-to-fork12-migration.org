To get started, let's follow the [[./deploy-using-sepolia.org]] document
to setup a fork 9 network running the legacy stack. From within the
repository root, I'm going to make a copy of the Sepolia template.

#+begin_src bash
cp .github/tests/external-l1/deploy-cdk-to-sepolia.yml sepolia-test.yml
#+end_src

In addition to configuring all of the values within that yaml file,
we'll also want to specify some additional settings to setup a legacy
fork 9 network. This is the filled out yaml that I'll use for my test.

#+begin_src yaml
deployment_stages:
  # Disable local L1.
  deploy_l1: false
  # We're doing legacy instead
  deploy_cdk_erigon_node: false

args:
  ## L1 Config
  l1_chain_id: 11155111
  # TODO: Create another mnemonic seed phrase for running the contract deployment on L1.
  l1_preallocated_mnemonic: close erosion wrist limb blur purpose shaft car myth shaft card spice
  # TODO: Adjust the amount of ETH you want to spend on this deployment.
  l1_funding_amount: 5ether
  # TODO: Configure the L1 RPC URLs to be valid Sepolia endpoints.
  l1_rpc_url: https://rpc.invalid@your-http-rpc-would-go-here/
  l1_ws_url: wss://rpc.invalid@your-ws-rpc-would-go-here/

  ## L2 Config
  # sequencer
  zkevm_l2_sequencer_address: "0x6884643521f317287A755cd41F159F51562CC358"
  zkevm_l2_sequencer_private_key: "0xefb7cabddc0b708fbc68835c797b933b6c73ffbf06d11a736b8b517751ac8086"

  # aggregator
  zkevm_l2_aggregator_address: "0x280D2d34ce497585A99C0B7013bed9AAd2fe55aF"
  zkevm_l2_aggregator_private_key: "0xa8a79524bb4e65b937290bb5b9de4359b2ea5b8814776f66e1c55ee925500266"

  # claimtxmanager
  zkevm_l2_claimtxmanager_address: "0x9a96D1b6BAd8b83ABC8C573eA9F741e7efef9Fff"
  zkevm_l2_claimtxmanager_private_key: "0x68d622e14edbb7fa74cd97c19a3c6e7be1b0c945ab999f425be80ff33795233f"

  # timelock
  zkevm_l2_timelock_address: "0xBA38e46f27d45822509a5628017A323124Ac1160"
  zkevm_l2_timelock_private_key: "0x5bd0ada480f2e23d39d712b4e1c16925f386bf3e284b9a84eae935023cd8aa1f"

  # admin
  zkevm_l2_admin_address: "0xAa27598c72ef76eF7FcBf46C8fD5DFEb2ab47De3"
  zkevm_l2_admin_private_key: "0xcd180929afb50de82238e2afcd062ae26292967091fedd58ca8216c1a4bc906f"

  # loadtest
  zkevm_l2_loadtest_address: "0xfB38CD9fa52475C10d739A76d2e4984E63F0d4F7"
  zkevm_l2_loadtest_private_key: "0xd4fb4cb16122d54adbdc423ee47e5f61eb7393e8bec46f7686b9a2d6ef20b4bb"

  # agglayer
  zkevm_l2_agglayer_address: "0x0472120b1f22027115B27475629ADF071C7DC9D7"
  zkevm_l2_agglayer_private_key: "0xddd32f8a8c7561c8d3abc3ebd48089a4b93358bd1846b1de03bc8d91824720c1"

  # dac
  zkevm_l2_dac_address: "0x5e8Fbd77B72d6004c8adD78F44Be7D92A89E78ac"
  zkevm_l2_dac_private_key: "0x344bf1c754a537d3bdcb764cd1c22627d9a29b779d782265daebaba0b5e3d9b9"

  # proofsigner
  zkevm_l2_proofsigner_address: "0x6822dB210A35024306d376cd12173aC1EA89C495"
  zkevm_l2_proofsigner_private_key: "0x3f8117c1df4fd13b62759d169e4b538ca47ea8772cefd5584c8b17047e20e69c"

  # l1testing
  zkevm_l2_l1testing_address: "0x522f428bFE3A094c9EC9Ca55190Dca171c5F2eD6"
  zkevm_l2_l1testing_private_key: "0x7c78663b8e014cf6682b08d985a502556da62e470768e0dfab09be584317d483"

  # claimsponsor
  zkevm_l2_claimsponsor_address: "0xdFF729179a7d33C3914237497813bD1Cda822717"
  zkevm_l2_claimsponsor_private_key: "0xe48071ef0c16a54cd99af5e8690da2ca1743f2d7bd0a431dbcf4024c519524a0"

  zkevm_contracts_image: leovct/zkevm-contracts:v6.0.0-rc.1-fork.9
  zkevm_prover_image: hermeznetwork/zkevm-prover:v6.0.8
  cdk_erigon_node_image: hermeznetwork/cdk-erigon:v2.1.2
  zkevm_node_image: hermeznetwork/zkevm-node:v0.7.3
  cdk_validium_node_image: 0xpolygon/cdk-validium-node:0.7.0-cdk
  zkevm_da_image: 0xpolygon/cdk-data-availability:0.0.10
  zkevm_bridge_service_image: hermeznetwork/zkevm-bridge-service:v0.6.0-RC1
  additional_services: []
  deploy_l2_contracts: true
  consensus_contract_type: cdk-validium
  sequencer_type: zkevm
#+end_src

The detailed process for setting up Sepolia are [[./deploy-using-sepolia.org][in the other document]],
so I'm not going to describe that here. Now that my network is
configured, I'm going to bring it up with kurtosis.

#+begin_src bash
kurtosis run --enclave cdk --args-file sepolia-test.yml .
#+end_src

During the deployment, I usually take note of the combined parameters:

#+begin_src javascript
{
  "polygonRollupManagerAddress": "0x7543107294e23136Bc493eDEBCCE9bd9690d4686",
  "polygonZkEVMBridgeAddress": "0xf47EA8C27Ca129B2b3fb7e6D3991d06Eac2BA728",
  "polygonZkEVMGlobalExitRootAddress": "0x351F6f6503D4f3c1ef8bFfa1Ba48D8052aF19c45",
  "polTokenAddress": "0xC58266da6A41Aa63e5FDA81345D1368357003F9c",
  "zkEVMDeployerContract": "0x0fAEF19A01b1B1CA3d09b46F227861422C45751D",
  "deployerAddress": "0xAa27598c72ef76eF7FcBf46C8fD5DFEb2ab47De3",
  "timelockContractAddress": "0x7CFda9704D616C4247F03008dFe28C1fad1B3c02",
  "deploymentRollupManagerBlockNumber": 7077492,
  "upgradeToULxLyBlockNumber": 7077492,
  "admin": "0xAa27598c72ef76eF7FcBf46C8fD5DFEb2ab47De3",
  "trustedAggregator": "0x280D2d34ce497585A99C0B7013bed9AAd2fe55aF",
  "proxyAdminAddress": "0xe477EdDe11F79E8C7a0e7108a5923864f5D7603D",
  "salt": "0x6e0d215341c328184a6f634169e29e223cb74efb907e94de3a2773f1c805cf08",
  "polygonDataCommitteeAddress": "0x95917C4aE989c4BF951cb244dc8249070c8519b3",
  "firstBatchData": {
    "transactions": "0xf9010380808401c9c38094f47ea8c27ca129b2b3fb7e6d3991d06eac2ba72880b8e4f811bff7000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000a40d5f56745a118d0906a34e69aec8c0db1cb8fa000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005ca1ab1e0000000000000000000000000000000000000000000000000000000005ca1ab1e1bff",
    "globalExitRoot": "0xad3228b676f7d3cd4284a5443f17f1962b36e491b30a40b2405849e597ba5fb5",
    "timestamp": 1731616152,
    "sequencer": "0x6884643521f317287A755cd41F159F51562CC358"
  },
  "genesis": "0x571931bf56aeb7c6326b279b5f41d8ed82cf35244879895124de3da59fd055e4",
  "createRollupBlockNumber": 7077496,
  "rollupAddress": "0x27215dfB58E4462f0114a69F9d7bDa52194095e2",
  "verifierAddress": "0x3C20e4b3931AbfFe13C7D50dFfdFFB3F9C127CBf",
  "consensusContract": "PolygonValidiumEtrog",
  "polygonZkEVML2BridgeAddress": "0xf47EA8C27Ca129B2b3fb7e6D3991d06Eac2BA728",
  "polygonZkEVMGlobalExitRootL2Address": "0xa40d5f56745a118d0906a34e69aec8c0db1cb8fa",
  "bridgeGenBlockNumber": 7077496
}
#+end_src

We can look here to see the verifications:
https://sepolia.etherscan.io/address/0x7543107294e23136Bc493eDEBCCE9bd9690d4686

Alright, I'm going to try to halt the sequencer now. First we should
get the current batch number:

#+begin_src bash
bn=$(cast rpc --rpc-url $(kurtosis port print cdk zkevm-node-sequencer-001 rpc) zkevm_batchNumber | jq -r)
printf "%d\n" $bn
#+end_src

Then we edit to edit the value of the ~HaltOnBatchNumber~ value within
the sequencer's config file. This is a little bit weird because we
have to use ~docker exec~ because we need to run the command with more
permissions that the ~zkevm-user~ will give us.

#+begin_src bash
docker exec --user root zkevm-node-sequencer-001--b1489f0f47f14ae79ae01b43579cab05 sed -i 's/HaltOnBatchNumber = 0/HaltOnBatchNumber = 14750/gi' /etc/zkevm/node-config.toml
kurtosis service stop cdk zkevm-node-sequencer-001
kurtosis service start cdk zkevm-node-sequencer-001
#+end_src

After we reach this block, we should be halted. We'll see logs like this

#+begin_example
{"level":"error","ts":1731691094.0874305,"caller":"sequencer/finalizer.go:905","msg":"halting finalizer, error: finalizer reached stop sequencer on batch number: 14750%!(EXTRA string=\n/home/runner/work/cdk-validium-node/cdk-validium-node/log/log.go:142 github.com/0xPolygonHermez/zkevm-node/log.appendStackTraceMaybeArgs()\n/home/runner/work/cdk-validium-node/cdk-validium-node/log/log.go:251 github.com/0xPolygonHermez/zkevm-node/log.Errorf()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:905 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).Halt()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/batch.go:272 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).closeAndOpenNewWIPBatch()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/batch.go:191 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).finalizeWIPBatch()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:463 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).finalizeBatches()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:184 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).Start()\n)","pid":7,"version":"0.7.0+cdk","stacktrace":"github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).Halt\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:905\ngithub.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).closeAndOpenNewWIPBatch\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/batch.go:272\ngithub.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).finalizeWIPBatch\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/batch.go:191\ngithub.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).finalizeBatches\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:463\ngithub.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).Start\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:184"}
#+end_example

Before proceeding, ideally all of the batch numbers are lined up:
#+begin_src bash
cast rpc --rpc-url $(kurtosis port print cdk zkevm-node-sequencer-001 rpc) zkevm_batchNumber
cast rpc --rpc-url $(kurtosis port print cdk zkevm-node-sequencer-001 rpc) zkevm_virtualBatchNumber
cast rpc --rpc-url $(kurtosis port print cdk zkevm-node-sequencer-001 rpc) zkevm_verifiedBatchNumber
#+end_src

Now that the network is halted and everything is aligned. Let's grab
the genesis file from our network and create a work directory

#+begin_src bash
work_dir=$(mktemp -d)
pushd $work_dir
kurtosis service exec cdk contracts-001 'cat /opt/zkevm/genesis.json' | tail -n +2 > genesis.json
kurtosis service exec cdk contracts-001 'cat /opt/zkevm/combined.json' | tail -n +2 > combined.json
#+end_src

We need to take these files and tweak them a bit specifically for erigon:

#+begin_src bash
mkdir conf
mkdir data
jq_script='
.genesis | map({
  (.address): {
    contractName: (if .contractName == "" then null else .contractName end),
    balance: (if .balance == "" then null else .balance end),
    nonce: (if .nonce == "" then null else .nonce end),
    code: (if .bytecode == "" then null else .bytecode end),
    storage: (if .storage == null or .storage == {} then null else (.storage | to_entries | sort_by(.key) | from_entries) end)
  }
}) | add'
batch_timestamp=$(jq '.firstBatchData.timestamp' combined.json)

jq "$jq_script" genesis.json > conf/dynamic-migrationexample-allocs.json
jq --arg bt "$batch_timestamp" '{"root": .root, "timestamp": ($bt | tonumber), "gasLimit": 0, "difficulty": 0}' genesis.json > conf/dynamic-migrationexample-conf.json
#+end_src

We'll need to create an additional file for the erigon chainspec. This file should be named ~dynamic-migrationexample-chainspec.json~:
#+begin_src bash
> conf/dynamic-migrationexample-chainspec.json cat <<EOF
{
  "ChainName": "dynamic-migrationexample",
  "chainId": 10101,
  "consensus": "ethash",
  "homesteadBlock": 0,
  "daoForkBlock": 0,
  "eip150Block": 0,
  "eip155Block": 0,
  "byzantiumBlock": 0,
  "constantinopleBlock": 0,
  "petersburgBlock": 0,
  "istanbulBlock": 0,
  "muirGlacierBlock": 0,
  "berlinBlock": 0,
  "londonBlock": 9999999999999999999999999999999999999999999999999,
  "arrowGlacierBlock": 9999999999999999999999999999999999999999999999999,
  "grayGlacierBlock": 9999999999999999999999999999999999999999999999999,
  "terminalTotalDifficulty": 58750000000000000000000,
  "terminalTotalDifficultyPassed": false,
  "shanghaiTime": 9999999999999999999999999999999999999999999999999,
  "cancunTime": 9999999999999999999999999999999999999999999999999,
  "normalcyBlock": 9999999999999999999999999999999999999999999999999,
  "pragueTime": 9999999999999999999999999999999999999999999999999,
  "ethash": {}
}
EOF
#+end_src

Now we need to generate a datastream file from the halted network:

#+begin_src bash
mkdir datafile
docker run -it -v $PWD/datafile:/datafile --network kt-cdk golang:1.23.3-bookworm
# run docker then do this stuff

cd
git clone https://github.com/0xPolygonHermez/zkevm-node.git
cd ~/zkevm-node/tools/datastreamer/
go build main.go

> config/tool.config.toml cat <<EOF
[Online]
URI = "localhost:6900"
StreamType = 1

[Offline]
Port = 6901
Filename = "datastream.bin"
Version = 4
ChainID = 1440
WriteTimeout = "5s"
InactivityTimeout = "120s"
InactivityCheckInterval = "5s"
UpgradeEtrogBatchNumber = 0

[StateDB]
User = "master_user"
Password = "master_password"
Name = "state_db"
Host = "postgres-001"
Port = "5432"
EnableLog = false
MaxConns = 200

[MerkleTree]
URI = ""
MaxThreads = 0
CacheFile = "merkle_tree_cache.json"

[Log]
Environment = "development"
Level = "error"
Outputs = ["stdout"]
EOF

make generate-file
cp -r datastream.* /datafile/
exit
#+end_src

Now we have a final snapshot of the chain. We're going to run a
datastream server based on the file that we just created:

#+begin_src bash
docker run --rm --name ds-host -it -v $PWD/datafile:/datafile --network kt-cdk golang:1.23.3-bookworm
# run docker then do this stuff
cd
git clone https://github.com/0xPolygonHermez/cdk-erigon.git
cd /root/cdk-erigon/zk/debug_tools/datastream-host
go run main.go --file /datafile/datastream.bin
#+end_src

We're also going to need a

Now that we have that running, we're going to create a configuration
file to guide erigon. It should be named
~dynamic-migrationexample.yaml~:

#+begin_src bash
> conf/dynamic-migrationexample.yaml cat <<EOF
datadir: /home/erigon/erigon-data
chain: dynamic-migrationexample
http: true

zkevm.l2-chain-id: 10101
zkevm.l2-sequencer-rpc-url: http://zkevm-node-sequencer-001:8123
zkevm.l2-datastreamer-url: ds-host:6900
zkevm.l1-chain-id: 11155111
zkevm.l1-rpc-url: https://rpc.sepolia.org

zkevm.address-sequencer: "0x6884643521f317287A755cd41F159F51562CC358"
zkevm.address-zkevm: "0x27215dfB58E4462f0114a69F9d7bDa52194095e2"
zkevm.address-rollup: "0x7543107294e23136Bc493eDEBCCE9bd9690d4686"
zkevm.address-ger-manager: "0x351F6f6503D4f3c1ef8bFfa1Ba48D8052aF19c45"

zkevm.default-gas-price: 1000000000
zkevm.max-gas-price: 0
zkevm.gas-price-factor: 0.12

zkevm.l1-rollup-id: 1
zkevm.l1-first-block: 7077496
zkevm.datastream-version: 3

externalcl: true
http.api: [eth, debug, net, trace, web3, erigon, zkevm]
http.addr: 0.0.0.0
http.vhosts: any
http.corsdomain: any
ws: true
EOF
#+end_src

#+begin_src bash
docker run --network kt-cdk \
    -v $PWD/data:/home/erigon/erigon-data \
    -v $PWD/conf:/home/erigon/dynamic-configs:ro hermeznetwork/cdk-erigon:v2.60.0-beta9 \
    --config /home/erigon/dynamic-configs/dynamic-migrationexample.yaml
#+end_src

There are some repeating logs like this once the node seems to be in caught up:

#+begin_example
[INFO] [11-15|18:49:34.160] [3/15 Batches] Waiting for at least one new block in datastream datastreamBlock=24578 last processed block=24578
[INFO] [11-15|18:49:44.192] [3/15 Batches] Waiting for at least one new block in datastream datastreamBlock=24578 last processed block=24578
[INFO] [11-15|18:49:54.225] [3/15 Batches] Waiting for at least one new block in datastream datastreamBlock=24578 last processed block=24578
[INFO] [11-15|18:50:04.250] [3/15 Batches] Waiting for at least one new block in datastream datastreamBlock=24578 last processed block=24578
#+end_example

We can stop erigon and we can stop the dshost node now. If you're
following along, the directly that you're working in shoud look a bit
like mine:

#+begin_example
├── combined.json
├── conf
│   ├── dynamic-migrationexample-allocs.json
│   ├── dynamic-migrationexample-chainspec.json
│   ├── dynamic-migrationexample-conf.json
│   └── dynamic-migrationexample.yaml
├── data
│   ├── caplin
│   │   ├── blobs
│   │   └── indexing
│   ├── chaindata
│   │   ├── mdbx.dat
│   │   └── mdbx.lck
│   ├── downloader
│   │   ├── mdbx.dat
│   │   └── mdbx.lck
│   ├── jwt.hex
│   ├── LOCK
│   ├── logs
│   │   └── cdk-erigon.log
│   ├── nodekey
│   ├── nodes
│   │   ├── eth67
│   │   │   ├── mdbx.dat
│   │   │   └── mdbx.lck
│   │   └── eth68
│   │       ├── mdbx.dat
│   │       └── mdbx.lck
│   ├── snapshots
│   │   ├── accessor
│   │   ├── domain
│   │   ├── history
│   │   ├── idx
│   │   └── prohibit_new_downloads.lock
│   ├── temp
│   └── txpool
│       ├── acls
│       │   ├── mdbx.dat
│       │   └── mdbx.lck
│       ├── mdbx.dat
│       └── mdbx.lck
├── datafile
│   ├── datastream.bin
│   └── datastream.db
│       ├── 000002.ldb
│       ├── 000003.log
│       ├── CURRENT
│       ├── CURRENT.bak
│       ├── LOCK
│       ├── LOG
│       └── MANIFEST-000004
└── genesis.json
#+end_example

We have a ~combined.json~ file and a ~genesis.json~ file along with a
datastream and an erigon data directory. This should be everything
that we need to spin up a new variation of the network after doing the
upgrade.

#+begin_src bash
docker exec -it contracts-001--43358fb3661f4e48a47b7566c600fec0 /bin/bash
# run the docker command first, but note the image name will be different

cd /opt/zkevm-contracts
git pull
git stash
git checkout v8.1.0-rc.1-fork.13
git stash apply
rm -rf artifacts cache node_modules
npm i

# confirm these with our own configs
rollup_manager_addr=0x7543107294e23136Bc493eDEBCCE9bd9690d4686
admin_private_key=0xcd180929afb50de82238e2afcd062ae26292967091fedd58ca8216c1a4bc906f

cat upgrade/upgradeBanana/upgrade_parameters.json.example |
    jq --arg rum $rollup_manager_addr \
       --arg sk $admin_private_key \
       --arg tld 60 '.rollupManagerAddress = $rum | .timelockDelay = $tld | .deployerPvtKey = $sk' > upgrade/upgradeBanana/upgrade_parameters.json

npx hardhat run ./upgrade/upgradeBanana/upgradeBanana.ts --network localhost
#+end_src

This command will print out two calldatas that are going to be used to
schedule and execute the upgrade against the timelock:

#+begin_example
{
  scheduleData: '0x8f2a0bb000000000000000000000000000000000000000000000000000000000000000c00000000000000000000000000000000000000000000000000000000000000120000000000000000000000000000000000000000000000000000000000000018000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003c0000000000000000000000000000000000000000000000000000000000000002000000000000000000000000e477edde11f79e8c7a0e7108a5923864f5d7603d000000000000000000000000e477edde11f79e8c7a0e7108a5923864f5d7603d00000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000a49623609d000000000000000000000000351f6f6503d4f3c1ef8bffa1ba48d8052af19c450000000000000000000000001eae51663b2b488e5e927eff48c3cc961fd4f70e000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000048129fc1c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004499a88ec40000000000000000000000007543107294e23136bc493edebcce9bd9690d4686000000000000000000000000ad4cc0b2f3149e61f8a2427ec86e07a403b5fd2f00000000000000000000000000000000000000000000000000000000'
}
{
  executeData: '0xe38335e500000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000160000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002000000000000000000000000e477edde11f79e8c7a0e7108a5923864f5d7603d000000000000000000000000e477edde11f79e8c7a0e7108a5923864f5d7603d00000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000a49623609d000000000000000000000000351f6f6503d4f3c1ef8bffa1ba48d8052af19c450000000000000000000000001eae51663b2b488e5e927eff48c3cc961fd4f70e000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000048129fc1c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004499a88ec40000000000000000000000007543107294e23136bc493edebcce9bd9690d4686000000000000000000000000ad4cc0b2f3149e61f8a2427ec86e07a403b5fd2f00000000000000000000000000000000000000000000000000000000'
}
#+end_example

Continuing on from within the contracts container, we're going to use
the two calldatas to schedule and then execute.

#+begin_src bash
time_lock_address="$(cat /opt/zkevm/combined.json | jq -r '.timelockContractAddress')"
private_key="0xcd180929afb50de82238e2afcd062ae26292967091fedd58ca8216c1a4bc906f"
rpc_url="https://rpc.invalid@your-http-rpc-would-go-here/"
schedule_data='0x8f2a0bb000000000000000000000000000000000000000000000000000000000000000c00000000000000000000000000000000000000000000000000000000000000120000000000000000000000000000000000000000000000000000000000000018000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003c0000000000000000000000000000000000000000000000000000000000000002000000000000000000000000e477edde11f79e8c7a0e7108a5923864f5d7603d000000000000000000000000e477edde11f79e8c7a0e7108a5923864f5d7603d00000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000a49623609d000000000000000000000000351f6f6503d4f3c1ef8bffa1ba48d8052af19c450000000000000000000000001eae51663b2b488e5e927eff48c3cc961fd4f70e000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000048129fc1c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004499a88ec40000000000000000000000007543107294e23136bc493edebcce9bd9690d4686000000000000000000000000ad4cc0b2f3149e61f8a2427ec86e07a403b5fd2f00000000000000000000000000000000000000000000000000000000'

cast send --rpc-url "$rpc_url" --private-key "$private_key" "$time_lock_address" "$schedule_data"
# 0x29ee9d20a059fcb3942783b12fbdcaac51adb132d113029f033aeee03d49c2ed

sleep 60

exec_data='0xe38335e500000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000160000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002000000000000000000000000e477edde11f79e8c7a0e7108a5923864f5d7603d000000000000000000000000e477edde11f79e8c7a0e7108a5923864f5d7603d00000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000a49623609d000000000000000000000000351f6f6503d4f3c1ef8bffa1ba48d8052af19c450000000000000000000000001eae51663b2b488e5e927eff48c3cc961fd4f70e000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000048129fc1c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004499a88ec40000000000000000000000007543107294e23136bc493edebcce9bd9690d4686000000000000000000000000ad4cc0b2f3149e61f8a2427ec86e07a403b5fd2f00000000000000000000000000000000000000000000000000000000'
cast send --rpc-url "$rpc_url" --private-key "$private_key" "$time_lock_address" "$exec_data"
# 0x9db29ceb9da8520636176cf21a7356bb31b90f922f5120cdb13f53dbbdf877d0
#+end_src

Now our rollup manager should be updated to Banana. This is needed for
compatibility with the latest versions of the CDK node and stuff like
that. Now we need to add a new rollup type for fork 12. The commands
below would still be executed from within the contracts image.

#+begin_src bash
genesis_root="$(cat /opt/zkevm/genesis.json | jq -r '.root')"
description="migrationexample genesis"

# We're going to use the SAME verifier for this test because it's use a mock prover here anyway
# If this were a real network, we'd need to deploy the fflonk 12 verifier
verifier_addr="$(cat /opt/zkevm/combined.json | jq -r '.verifierAddress')"
cp /opt/zkevm/genesis.json tools/addRollupType/genesis.json

cat tools/addRollupType/add_rollup_type.json.example |
    jq --arg rum $rollup_manager_addr \
       --arg sk $admin_private_key \
       --arg gr $genesis_root \
       --arg vf $verifier_addr \
       --arg desc "$description" \
       --arg tld 60 '
           .polygonRollupManagerAddress = $rum |
           .timelockDelay = $tld |
           .deployerPvtKey = $sk |
           .forkID = 12 |
           .genesisRoot = $gr |
           .description = $desc |
           .verifierAddress = $vf' > tools/addRollupType/add_rollup_type.json

npx hardhat run ./tools/addRollupType/addRollupType.ts --network localhost
#+end_src

Since this is a test deployment, it looks like thie script added to
rollup type directly without going through the time lock.

https://sepolia.etherscan.io/tx/0x862fdb381b954fba957b48c58136a4df911aa9c699367f01f9254e3e96e7f719

Now we get to do the same thing (pretty much) one more time to make
sure that the rollup is updated to the new type.

#+begin_src bash
rollup_addr="$(cat /opt/zkevm/combined.json | jq -r '.rollupAddress')"

cat tools/updateRollup/updateRollup.json.example |
    jq --arg rum $rollup_manager_addr \
       --arg sk $admin_private_key \
       --arg ru $rollup_addr \
       --arg tld 60 '
           .polygonRollupManagerAddress = $rum |
           .timelockDelay = $tld |
           .deployerPvtKey = $sk |
           .newRollupTypeID = 3 |
           .rollupAddress = $ru' > tools/updateRollup/updateRollup.json

npx hardhat run ./tools/updateRollup/updateRollup.ts --network localhost
#+end_src

This looks to have been executed successuflly on chain as well:

https://sepolia.etherscan.io/tx/0x097d8563da2512c1e4a6c54f086fe792b354e474e051b8ee0b965bd28cda959c

In my very specific case, I'm going to need to update the trusted sequencer url because it's going to change from ~zkevm-node-sequencer-001~ to ~cdk-erigon-sequencer-001~.

#+begin_src bash
rpc_url="https://rpc.invalid@your-http-rpc-would-go-here/"
private_key="0xcd180929afb50de82238e2afcd062ae26292967091fedd58ca8216c1a4bc906f"
rollup_addr="$(cat /opt/zkevm/combined.json | jq -r '.rollupAddress')"

cast send --private-key "$private_key" --rpc-url "$rpc_url" "$rollup_addr" 'setTrustedSequencerURL(string)' http://cdk-erigon-sequencer-001:8123
#+end_src

Now the trusted sequencer url should be updated to work with Erigon
rather than the legacy node. If the old infra is still running, this
should stop it.

https://sepolia.etherscan.io/tx/0xeb24460feccb5a05580bfe0df90754a95368194ab94c02c8eae6dbdaeee84e1c

At this point, we should be in a good position to destroy the original
enclave and bring up a new one that is attached. Just in case anything
goes wrong, I'm going to leave the original enclave alone and bring up
a new one with the upgraded components. We'll need to make some
modifications to the yaml file. I'm going to run this command from the
root of the Kurtosis CDK repo.

#+begin_src yaml
yq -y '
  .args.zkevm_contracts_image = "leovct/zkevm-contracts:v8.0.0-rc.4-fork.12" |
  .args.zkevm_prover_image = "hermeznetwork/zkevm-prover:v8.0.0-RC14-fork.12" |
  .args.cdk_erigon_node_image = "hermeznetwork/cdk-erigon:v2.60.0-beta8" |
  .args.zkevm_da_image = "0xpolygon/cdk-data-availability:0.0.10" |
  .args.zkevm_bridge_service_image = "hermeznetwork/zkevm-bridge-service:v0.6.0-RC1" |
  .args.consensus_contract_type = "cdk-validium" |
  .args.sequencer_type = "erigon" |
  .deployment_stages.deploy_cdk_erigon_node = true |
  .args.use_previously_deployed_contracts = true |
  .args.erigon_datadir_archive = "../templates/contract-deploy/erigon-data" |
  .args.chain_name = "migrationexample"
' sepolia-test.yml > sepolia-erigon-test.yml

# This is the temp directory that we made earlier
cp /tmp/tmp.XKAVPPJ6VF/genesis.json templates/contract-deploy/genesis.json
cp /tmp/tmp.XKAVPPJ6VF/combined.json templates/contract-deploy/combined.json
cp /tmp/tmp.XKAVPPJ6VF/conf/* templates/contract-deploy/
cp -r /tmp/tmp.XKAVPPJ6VF/data templates/contract-deploy/erigon-data

# try to spin up the new network
kurtosis run --enclave cdk2 --args-file sepolia-erigon-test.yml .
#+end_src

At this point the sequencer is running and after pointing the cdk node
to the sequencer. We've seen a batch get sequenced and finalized on L2:

- https://sepolia.etherscan.io/tx/0x63ddc2886d088c66bb51d22b8d7699b6da5d5f13edaeed5c57b34e5389084717

* TODO Re-run this with the latest beta
* TODO Understand missing acc input hash errors
* TODO Resolve issue with RPC node not syncing
* TODO Resolve this error

#+begin_example
2024-11-16T15:30:51.885Z	ERROR	aggregator/aggregator.go:1215	Failed to get witness for batch 8253, err: error from witness for batch 8253: &{-32000 block number is in the future latest=7789 requested=13753 <nil>}	{"pid": 9, "version": "v0.4.0-beta8", "module": "aggregator"}
github.com/0xPolygon/cdk/aggregator.(*Aggregator).getAndLockBatchToProve
	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:1215
github.com/0xPolygon/cdk/aggregator.(*Aggregator).tryGenerateBatchProof
	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:1278
github.com/0xPolygon/cdk/aggregator.(*Aggregator).Channel
	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:453
github.com/0xPolygon/cdk/aggregator/prover._AggregatorService_Channel_Handler
	/go/src/github.com/0xPolygon/cdk/aggregator/prover/aggregator_grpc.pb.go:109
google.golang.org/grpc.(*Server).processStreamingRPC
	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1673
google.golang.org/grpc.(*Server).handleStream
	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1794
google.golang.org/grpc.(*Server).serveStreams.func2.1
	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1029
#+end_example

