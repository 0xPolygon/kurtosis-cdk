This document details an upgrade process using Kurtosis where we move
from fork 9 using the legacy zkevm node to fork 12 using
cdk-erigon. The process here isn't exactly suitable for production
environments, but it would give you a detailed sense of the steps and
tools required.

** Initial Fork 9 Setup

To get started, let's follow the [[./deploy-using-sepolia.org]] document
to setup a fork 9 network running the legacy stack. From within the
repository root, I'm going to make a copy of the Sepolia args
template.

#+begin_src bash
cp .github/tests/external-l1/deploy-cdk-to-sepolia.yml sepolia-test.yml
#+end_src

In addition to configuring all of the values within that yaml file,
we'll also want to specify some additional settings to setup a legacy
fork 9 network. This is the filled out yaml that I'll use for my test.

#+begin_src yaml
deployment_stages:
  # Disable local L1.
  deploy_l1: false
  # We're doing legacy instead
  deploy_cdk_erigon_node: false

args:
  ## L1 Config
  l1_chain_id: 11155111
  # TODO: Create another mnemonic seed phrase for running the contract deployment on L1.
  l1_preallocated_mnemonic: mesh addict trick marine among trash bamboo fabric obvious school blast field
  # TODO: Adjust the amount of ETH you want to spend on this deployment.
  l1_funding_amount: 5ether
  # TODO: Configure the L1 RPC URLs to be valid Sepolia endpoints.
  l1_rpc_url: https://rpc.invalid@your-http-rpc-would-go-here/
  l1_ws_url: wss://rpc.invalid@your-ws-rpc-would-go-here/

  ## L2 Config
  # sequencer
  l2_sequencer_address: "0x99315dc829d9C54bdA511a3fF544733025041Ee2"
  l2_sequencer_private_key: "0xdabba7cf6c9eba30b56cf1d5764e86b5d4e4f89e560e4930a750e2cd1f7b84e7"

  # aggregator
  l2_aggregator_address: "0x85dd37b4DbBdEB0Ff2ad6e717C2BbA18a2eD4B03"
  l2_aggregator_private_key: "0xa7a66b300528fa1f0bfe73c0ece1ff59c51b68bf2d1e7ac2511a2496960e14b7"

  # admin
  l2_admin_address: "0x3802C134A821874a00363c72d4EA0aCAa5321627"
  l2_admin_private_key: "0x163d6d315db71700ad91eae84439a4cd1afa6f80372a84859070672c0a8bca04"

  # dac
  l2_dac_address: "0xAaB75050a8981a1C90e7fec940D51058127Af425"
  l2_dac_private_key: "0xefa54be13b9df9fdec01a3dbe1538a7998859a946cf2b56ca42d11a795d8e8ac"

  # claimsponsor
  l2_claimsponsor_address: "0xF5BED6F96394A054553a037743fdbfc6e6065A4d"
  l2_claimsponsor_private_key: "0xe84cf50e79539b794f2f8f4879c8742e71b1e72f74e6863786ac95d6b60ac33c"

  # I've appended this section to make sure that we deploy a legacy stack first
  zkevm_contracts_image: leovct/zkevm-contracts:v6.0.0-rc.1-fork.9
  zkevm_prover_image: hermeznetwork/zkevm-prover:v6.0.8
  cdk_erigon_node_image: hermeznetwork/cdk-erigon:v2.1.2
  zkevm_node_image: hermeznetwork/zkevm-node:v0.7.3
  cdk_validium_node_image: 0xpolygon/cdk-validium-node:0.7.0-cdk
  zkevm_da_image: 0xpolygon/cdk-data-availability:0.0.11
  zkevm_bridge_service_image: hermeznetwork/zkevm-bridge-service:v0.6.0-RC7
  additional_services: []
  deploy_l2_contracts: true
  consensus_contract_type: cdk_validium
  sequencer_type: zkevm
#+end_src

The detailed process for setting up Sepolia are [[./deploy-using-sepolia.org][in the other document]],
so I'm not going to describe that here. Now that my network is
configured, I'm going to bring it up with kurtosis.

#+begin_src bash
kurtosis run --enclave cdk --args-file sepolia-test.yml .
#+end_src

During the deployment, I usually take note of the combined
parameters. This gets logged out during the run, but you can also
print it directly with a call like this:

#+begin_src bash
kurtosis service exec cdk contracts-001 'cat /opt/output/combined.json'
#+end_src

#+begin_src javascript
{
  "polygonRollupManagerAddress": "0x886f280F1c69c8914AC82496b937af293a3D4586",
  "polygonZkEVMBridgeAddress": "0xa49CA87527D0c3aB52F94dE4fa96d8F614CDB962",
  "polygonZkEVMGlobalExitRootAddress": "0xE0584840CF77A8876FA261155D7e397987d7C083",
  "polTokenAddress": "0x5C40604773F62EaDDfbf920B2EDb00e79cBE576A",
  "zkEVMDeployerContract": "0x22c4B5E194aF8fA0Ed8d5C2f2cC7316f4E3c6148",
  "deployerAddress": "0x3802C134A821874a00363c72d4EA0aCAa5321627",
  "timelockContractAddress": "0x732ad44A7762e6Fce740A787dfC2BAf8259Da431",
  "deploymentRollupManagerBlockNumber": 7105509,
  "upgradeToULxLyBlockNumber": 7105509,
  "admin": "0x3802C134A821874a00363c72d4EA0aCAa5321627",
  "trustedAggregator": "0x85dd37b4DbBdEB0Ff2ad6e717C2BbA18a2eD4B03",
  "proxyAdminAddress": "0x16d661ed65b25Af5bf96301fed9C6D093B631334",
  "salt": "0x0d0d16b63f1f371da0a4f83f5083dbc3810f6fae3cb44a108aac7fdc9f062d2e",
  "polygonDataCommitteeAddress": "0x2cd07AE9bd9b2653E20c07DEDD5232436F0AF4ef",
  "firstBatchData": {
    "transactions": "0xf9010380808401c9c38094a49ca87527d0c3ab52f94de4fa96d8f614cdb96280b8e4f811bff7000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000a40d5f56745a118d0906a34e69aec8c0db1cb8fa000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005ca1ab1e0000000000000000000000000000000000000000000000000000000005ca1ab1e1bff",
    "globalExitRoot": "0xad3228b676f7d3cd4284a5443f17f1962b36e491b30a40b2405849e597ba5fb5",
    "timestamp": 1731974160,
    "sequencer": "0x99315dc829d9C54bdA511a3fF544733025041Ee2"
  },
  "genesis": "0x57d59198b730bb440a025eda68b0ad6881d8245bd487c296fe9ee7449578c314",
  "createRollupBlockNumber": 7105513,
  "rollupAddress": "0xB7464021cC6C343A63915bB30b627B88FBB0df91",
  "verifierAddress": "0x8C978Eaf0398dCf95985105a548538eb57d1fEcb",
  "consensusContract": "PolygonValidiumEtrog",
  "polygonZkEVML2BridgeAddress": "0xa49CA87527D0c3aB52F94dE4fa96d8F614CDB962",
  "polygonZkEVMGlobalExitRootL2Address": "0xa40d5f56745a118d0906a34e69aec8c0db1cb8fa",
  "bridgeGenBlockNumber": 7105513
}
#+end_src

This file has all of the critical contract values that would have been
deployed. In order to check the verifications, we can look at the
rollup manager address in the explorer:
https://sepolia.etherscan.io/address/0x886f280F1c69c8914AC82496b937af293a3D4586

At this point your network should be up and running and mining blocks
and transactions. Just to confirm, we can attempt to send a
transaction like this:

#+begin_src bash
# this is using the admin private key
cast send --legacy --private-key "0x163d6d315db71700ad91eae84439a4cd1afa6f80372a84859070672c0a8bca04" --rpc-url $(kurtosis port print cdk zkevm-node-rpc-001 rpc) --value 0.01ether 0x0000000000000000000000000000000000000000
#+end_src

** Performing L1 Upgrades

The first thing we'll want to do is halt the sequencer. First we
should get the current batch number:

#+begin_src bash
bn=$(cast rpc --rpc-url $(kurtosis port print cdk zkevm-node-sequencer-001 rpc) zkevm_batchNumber | jq -r)
printf "%d\n" $bn
#+end_src

Then we edit to edit the value of the ~HaltOnBatchNumber~ value within
the sequencer's config file. This is a little bit weird because we
have to use ~docker exec~ because we need to run the command with more
permissions that the ~zkevm-user~ will give us.

#+begin_src bash
docker exec --user root zkevm-node-sequencer-001- sed -i 's/HaltOnBatchNumber = 0/HaltOnBatchNumber = 100/gi' /etc/zkevm/node-config.toml
kurtosis service stop cdk zkevm-node-sequencer-001
kurtosis service start cdk zkevm-node-sequencer-001
#+end_src

After we reach batch 100, the sequencer should be halted. We can
confirm that via the logs:

#+begin_src bash
kurtosis service logs --follow cdk zkevm-node-sequencer-001
#+end_src

#+begin_example
[zkevm-node-sequencer-001] {"level":"error","ts":1731975974.9684155,"caller":"sequencer/finalizer.go:905","msg":"halting finalizer, error: finalizer reached stop sequencer on batch number: 100%!(EXTRA string=\n/home/runner/work/cdk-validium-node/cdk-validium-node/log/log.go:142 github.com/0xPolygonHermez/zkevm-node/log.appendStackTraceMaybeArgs()\n/home/runner/work/cdk-validium-node/cdk-validium-node/log/log.go:251 github.com/0xPolygonHermez/zkevm-node/log.Errorf()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:905 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).Halt()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/batch.go:272 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).closeAndOpenNewWIPBatch()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/batch.go:191 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).finalizeWIPBatch()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:463 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).finalizeBatches()\n/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:184 github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).Start()\n)","pid":7,"version":"0.7.0+cdk","stacktrace":"github.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).Halt\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:905\ngithub.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).closeAndOpenNewWIPBatch\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/batch.go:272\ngithub.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).finalizeWIPBatch\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/batch.go:191\ngithub.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).finalizeBatches\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:463\ngithub.com/0xPolygonHermez/zkevm-node/sequencer.(*finalizer).Start\n\t/home/runner/work/cdk-validium-node/cdk-validium-node/sequencer/finalizer.go:184"}

#+end_example

Alright, it looks like we have halted successfully. This means the
sequencer is no longer creating new blocks or batches. Before
proceeding, ideally all of the batch numbers are lined up:
#+begin_src bash
cast rpc --rpc-url $(kurtosis port print cdk zkevm-node-sequencer-001 rpc) zkevm_batchNumber
cast rpc --rpc-url $(kurtosis port print cdk zkevm-node-sequencer-001 rpc) zkevm_virtualBatchNumber
cast rpc --rpc-url $(kurtosis port print cdk zkevm-node-sequencer-001 rpc) zkevm_verifiedBatchNumber
#+end_src

When all three of those numbers are aligned, it means that the state
of the L2 is completely verified on L1 and we're in a good position to
perform the upgrade. Depending on how your network is configured this
could take a while. There are some settings that can be adjusted to
speed up the aggregation interval, sequence sender period, and also
the L1 finalization requirements.

We're going to pull a bunch of data from the existing network in order
to get ready for the upgrade. First we'll grab the genesis file along
with the combined.json:

#+begin_src bash
work_dir=$(mktemp -d)
pushd $work_dir
kurtosis service exec cdk contracts-001 'cat /opt/output/genesis.json' | tail -n +2 > genesis.json
kurtosis service exec cdk contracts-001 'cat /opt/output/combined.json' | tail -n +2 > combined.json
#+end_src

This files will need to be tweaked for Erigon compatibility.

#+begin_src bash
mkdir conf
mkdir data
jq_script='
.genesis | map({
  (.address): {
    contractName: (if .contractName == "" then null else .contractName end),
    balance: (if .balance == "" then null else .balance end),
    nonce: (if .nonce == "" then null else .nonce end),
    code: (if .bytecode == "" then null else .bytecode end),
    storage: (if .storage == null or .storage == {} then null else (.storage | to_entries | sort_by(.key) | from_entries) end)
  }
}) | add'
batch_timestamp=$(jq '.firstBatchData.timestamp' combined.json)

jq "$jq_script" genesis.json > conf/dynamic-migrationexample-allocs.json
jq --arg bt "$batch_timestamp" '{"root": .root, "timestamp": ($bt | tonumber), "gasLimit": 0, "difficulty": 0}' genesis.json > conf/dynamic-migrationexample-conf.json
#+end_src

We'll also need to create an additional file for the Erigon chainspec. This
file should be named ~dynamic-migrationexample-chainspec.json~:

#+begin_src bash
> conf/dynamic-migrationexample-chainspec.json cat <<EOF
{
  "ChainName": "dynamic-migrationexample",
  "chainId": 10101,
  "consensus": "ethash",
  "homesteadBlock": 0,
  "daoForkBlock": 0,
  "eip150Block": 0,
  "eip155Block": 0,
  "byzantiumBlock": 0,
  "constantinopleBlock": 0,
  "petersburgBlock": 0,
  "istanbulBlock": 0,
  "muirGlacierBlock": 0,
  "berlinBlock": 0,
  "londonBlock": 9999999999999999999999999999999999999999999999999,
  "arrowGlacierBlock": 9999999999999999999999999999999999999999999999999,
  "grayGlacierBlock": 9999999999999999999999999999999999999999999999999,
  "terminalTotalDifficulty": 58750000000000000000000,
  "terminalTotalDifficultyPassed": false,
  "shanghaiTime": 9999999999999999999999999999999999999999999999999,
  "cancunTime": 9999999999999999999999999999999999999999999999999,
  "normalcyBlock": 9999999999999999999999999999999999999999999999999,
  "pragueTime": 9999999999999999999999999999999999999999999999999,
  "ethash": {}
}
EOF
#+end_src

Now we need to generate a datastream file from the halted network. The
process below will first start a container that is attached to the
network we created. Then the rest of the commands are actually
generating the datastream:

#+begin_src bash
mkdir datafile
docker run -it -v $PWD/datafile:/datafile --network kt-cdk golang:1.23.3-bookworm
# run docker then do this stuff

cd
git clone https://github.com/0xPolygonHermez/zkevm-node.git
cd ~/zkevm-node/tools/datastreamer/
go build main.go

> config/tool.config.toml cat <<EOF
[Online]
URI = "localhost:6900"
StreamType = 1

[Offline]
Port = 6901
Filename = "datastream.bin"
Version = 4
ChainID = 1440
WriteTimeout = "5s"
InactivityTimeout = "120s"
InactivityCheckInterval = "5s"
UpgradeEtrogBatchNumber = 0

[StateDB]
User = "master_user"
Password = "master_password"
Name = "state_db"
Host = "postgres-001"
Port = "5432"
EnableLog = false
MaxConns = 200

[MerkleTree]
URI = ""
MaxThreads = 0
CacheFile = "merkle_tree_cache.json"

[Log]
Environment = "development"
Level = "error"
Outputs = ["stdout"]
EOF

make generate-file
cp -r datastream.* /datafile/
exit
#+end_src

Essentially what this block is doing is:
- Creating a ~datafile~ directory to hold the data
- Attaching a generic go container to the network
- Building a tool to regenerate the datafile directly from the state
  database
- Configuring and running that tool
- Saving the data to the mounted path

The zkevm-node runs a datastream, but the format is incompatible with
erigon, so we need to use this tool to generate a static file. This
process is also good from a backup / recovery perspective.

Optionally, I'm also going to take a snapshot of the DAC DB. This
shouldn't strictly speaking be necessary because the DAC can rebuild
it's data from a trusted source. But it's good to have in the case of
a validium because it's the primary source of data.

#+begin_src bash
# in the case of kurtosis the password is master_password
pg_dump -U master_user -d dac_db -h 127.0.0.1 -p $(kurtosis port print cdk postgres-001 postgres | sed 's/.*://') > dac.db.sql
#+end_src

Now we have a final snapshot of the chain. We're going to run a
datastream server based on the file that we just created:

#+begin_src bash
docker run --rm --name ds-host -it -v $PWD/datafile:/datafile --network kt-cdk golang:1.23.3-bookworm
# run docker then do this stuff
cd
git clone https://github.com/0xPolygonHermez/cdk-erigon.git
cd /root/cdk-erigon/zk/debug_tools/datastream-host
go run main.go --file /datafile/datastream.bin
#+end_src

Now that we have that running, we're going to create a configuration
file to guide erigon. It should be named
~dynamic-migrationexample.yaml~. The goal here is to take the
datastream that we generated from the legacy network and get erigon to
build it's own data directory based on that datastream.

#+begin_src bash
> conf/dynamic-migrationexample.yaml cat <<EOF
datadir: /home/erigon/erigon-data
chain: dynamic-migrationexample
http: true

zkevm.l2-chain-id: 10101
zkevm.l2-sequencer-rpc-url: http://zkevm-node-sequencer-001:8123
zkevm.l2-datastreamer-url: ds-host:6900
zkevm.l1-chain-id: 11155111
zkevm.l1-rpc-url: https://rpc.sepolia.org

# these values need to be changed!
zkevm.address-sequencer: "0x99315dc829d9C54bdA511a3fF544733025041Ee2"
zkevm.address-zkevm: "0x50222266502996951a1Db4D2eb154dc7441d7565" # rollupAddress
zkevm.address-rollup: "0xd82c6C3CaD564623B1AECd616A302D856e9BdAD0" # polygonRollupManagerAddress
zkevm.address-ger-manager: "0x482E2568647269051944Cb8894311fd90a68516b" # polygonZkEVMGlobalExitRootAddress

zkevm.default-gas-price: 1000000000
zkevm.max-gas-price: 0
zkevm.gas-price-factor: 0.12

zkevm.l1-rollup-id: 1
zkevm.l1-first-block: 7077496
zkevm.datastream-version: 3

externalcl: true
http.api: [eth, debug, net, trace, web3, erigon, zkevm]
http.addr: 0.0.0.0
http.vhosts: any
http.corsdomain: any
ws: true
EOF
#+end_src

#+begin_src bash
docker run --network kt-cdk \
    -v $PWD/data:/home/erigon/erigon-data \
    -v $PWD/conf:/home/erigon/dynamic-configs:ro hermeznetwork/cdk-erigon:v2.60.0-beta10 \
    --config /home/erigon/dynamic-configs/dynamic-migrationexample.yaml
#+end_src

There are some repeating logs like this once the node seems to be in caught up:

#+begin_example
[INFO] [11-19|00:32:54.749] [3/15 Batches] Waiting for at least one new block in datastream datastreamBlock=164 last processed block=164
[INFO] [11-19|00:33:04.786] [3/15 Batches] Waiting for at least one new block in datastream datastreamBlock=164 last processed block=164
#+end_example

If you're following along, the directory that you're working in should
look a bit like mine:

#+begin_example
├── combined.json
├── conf
│   ├── dynamic-migrationexample-allocs.json
│   ├── dynamic-migrationexample-chainspec.json
│   ├── dynamic-migrationexample-conf.json
│   └── dynamic-migrationexample.yaml
├── dac.db.sql
├── data
│   ├── caplin
│   │   ├── blobs
│   │   └── indexing
│   ├── chaindata
│   │   ├── mdbx.dat
│   │   └── mdbx.lck
│   ├── downloader
│   │   ├── mdbx.dat
│   │   └── mdbx.lck
│   ├── jwt.hex
│   ├── LOCK
│   ├── logs
│   │   └── cdk-erigon.log
│   ├── nodekey
│   ├── nodes
│   │   ├── eth67
│   │   │   ├── mdbx.dat
│   │   │   └── mdbx.lck
│   │   └── eth68
│   │       ├── mdbx.dat
│   │       └── mdbx.lck
│   ├── snapshots
│   │   ├── accessor
│   │   ├── domain
│   │   ├── history
│   │   ├── idx
│   │   └── prohibit_new_downloads.lock
│   ├── temp
│   └── txpool
│       ├── acls
│       │   ├── mdbx.dat
│       │   └── mdbx.lck
│       ├── mdbx.dat
│       └── mdbx.lck
├── datafile
│   ├── datastream.bin
│   └── datastream.db
│       ├── 000002.ldb
│       ├── 000003.log
│       ├── CURRENT
│       ├── CURRENT.bak
│       ├── LOCK
│       ├── LOG
│       └── MANIFEST-000004
└── genesis.json
#+end_example

We have a ~combined.json~ file and a ~genesis.json~ file along with a
datastream and an erigon data directory. This should be everything
that we need to spin up a new variation of the network after doing the
upgrade.

Now we're going to do a bunch of contract work and upgrades. This is
all done within docker for convenience as well.

#+begin_src bash
docker exec -it contracts-001--abc8888306a74c7fa5148c0c3ce5e796 /bin/bash
# run the docker command first, but note the image name will be different

cd /opt/agglayer-contracts
git pull
git stash
git checkout v8.1.0-rc.1-fork.13
git stash apply
rm -rf artifacts cache node_modules
npm i

# confirm these with our own configs
rollup_manager_addr="$(cat /opt/output/combined.json | jq -r '.polygonRollupManagerAddress')"
admin_private_key="$(cat deployment/v2/deploy_parameters.json | jq -r '.deployerPvtKey')"

cat upgrade/upgradeBanana/upgrade_parameters.json.example |
    jq --arg rum $rollup_manager_addr \
       --arg sk $admin_private_key \
       --arg tld 60 '.rollupManagerAddress = $rum | .timelockDelay = $tld | .deployerPvtKey = $sk' > upgrade/upgradeBanana/upgrade_parameters.json

npx hardhat run ./upgrade/upgradeBanana/upgradeBanana.ts --network localhost
#+end_src

This command will print out two calldatas that are going to be used to
schedule and execute the upgrade via the timelock:

#+begin_example
{
  scheduleData: '0x8f2a0bb000000000000000000000000000000000000000000000000000000000000000c00000000000000000000000000000000000000000000000000000000000000120000000000000000000000000000000000000000000000000000000000000018000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003c000000000000000000000000000000000000000000000000000000000000000200000000000000000000000016d661ed65b25af5bf96301fed9c6d093b63133400000000000000000000000016d661ed65b25af5bf96301fed9c6d093b63133400000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000a49623609d000000000000000000000000e0584840cf77a8876fa261155d7e397987d7c08300000000000000000000000066358fe47ee8a52cc080155d826f4d78a4e544d3000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000048129fc1c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004499a88ec4000000000000000000000000886f280f1c69c8914ac82496b937af293a3d45860000000000000000000000009a26ea1e13b396b07dcc09a6416cc8223458782000000000000000000000000000000000000000000000000000000000'
}
{
  executeData: '0xe38335e500000000000000000000000000000000000000000000000000000000000000a00000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000016000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000200000000000000000000000016d661ed65b25af5bf96301fed9c6d093b63133400000000000000000000000016d661ed65b25af5bf96301fed9c6d093b63133400000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000a49623609d000000000000000000000000e0584840cf77a8876fa261155d7e397987d7c08300000000000000000000000066358fe47ee8a52cc080155d826f4d78a4e544d3000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000048129fc1c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004499a88ec4000000000000000000000000886f280f1c69c8914ac82496b937af293a3d45860000000000000000000000009a26ea1e13b396b07dcc09a6416cc8223458782000000000000000000000000000000000000000000000000000000000'
}
#+end_example

Continuing on from within the contracts container, we're going to use
the two calldatas to schedule and then execute.

#+begin_src bash
time_lock_address="$(cat /opt/output/combined.json | jq -r '.timelockContractAddress')"
private_key="$(cat deployment/v2/deploy_parameters.json | jq -r '.deployerPvtKey')"

# rpc_url="http://172.19.0.2:8545/"
rpc_url="https://rpc.invalid@your-http-rpc-would-go-here/"
schedule_data='0x8f2a0bb000000000000000000000000000000000000000000000000000000000000000c00000000000000000000000000000000000000000000000000000000000000120000000000000000000000000000000000000000000000000000000000000018000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003c000000000000000000000000000000000000000000000000000000000000000200000000000000000000000016d661ed65b25af5bf96301fed9c6d093b63133400000000000000000000000016d661ed65b25af5bf96301fed9c6d093b63133400000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000a49623609d000000000000000000000000e0584840cf77a8876fa261155d7e397987d7c08300000000000000000000000066358fe47ee8a52cc080155d826f4d78a4e544d3000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000048129fc1c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004499a88ec4000000000000000000000000886f280f1c69c8914ac82496b937af293a3d45860000000000000000000000009a26ea1e13b396b07dcc09a6416cc8223458782000000000000000000000000000000000000000000000000000000000'

cast send --rpc-url "$rpc_url" --private-key "$private_key" "$time_lock_address" "$schedule_data"
# 0x649fafbafd7b6b4b27143bbcbd29ee3eec4f68865a9fabfebaa5a18957822cd2

sleep 60

exec_data='0xe38335e500000000000000000000000000000000000000000000000000000000000000a00000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000016000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000200000000000000000000000016d661ed65b25af5bf96301fed9c6d093b63133400000000000000000000000016d661ed65b25af5bf96301fed9c6d093b63133400000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000a49623609d000000000000000000000000e0584840cf77a8876fa261155d7e397987d7c08300000000000000000000000066358fe47ee8a52cc080155d826f4d78a4e544d3000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000048129fc1c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004499a88ec4000000000000000000000000886f280f1c69c8914ac82496b937af293a3d45860000000000000000000000009a26ea1e13b396b07dcc09a6416cc8223458782000000000000000000000000000000000000000000000000000000000'
cast send --rpc-url "$rpc_url" --private-key "$private_key" "$time_lock_address" "$exec_data"
# 0xd6a6ee2c82eec970ea88b9743012b5513a8901f9b417d4b0dae09c762092b7cb
#+end_src

Now our rollup manager should be updated to Banana. This is needed for
compatibility with the latest versions of the CDK node and stuff like
that. Now we need to add a new rollup type for fork 12. The commands
below would still be executed from within the contracts image.

#+begin_src bash
genesis_root="$(cat /opt/output/genesis.json | jq -r '.root')"
description="migrationexample genesis"

# We're going to use the SAME verifier for this test because it's use a mock prover here anyway
# If this were a real network, we'd need to deploy the fflonk 12 verifier
verifier_addr="$(cat /opt/output/combined.json | jq -r '.verifierAddress')"
cp /opt/output/genesis.json tools/addRollupType/genesis.json

cat tools/addRollupType/add_rollup_type.json.example |
    jq --arg rum $rollup_manager_addr \
       --arg sk $admin_private_key \
       --arg gr $genesis_root \
       --arg vf $verifier_addr \
       --arg desc "$description" \
       --arg tld 60 '
           .polygonRollupManagerAddress = $rum |
           .timelockDelay = $tld |
           .deployerPvtKey = $sk |
           .forkID = 12 |
           .genesisRoot = $gr |
           .description = $desc |
           .verifierAddress = $vf' > tools/addRollupType/add_rollup_type.json

npx hardhat run ./tools/addRollupType/addRollupType.ts --network localhost
#+end_src

Since this is a test deployment, it looks like the script added the
rollup type directly without going through the time lock.

https://sepolia.etherscan.io/tx/0x2e2996f6fd4ec9152d26db6a8cce9771cbaf8cc0a2a71f857cec935745bf0029

Now we get to do the same thing (pretty much) one more time to make
sure that the rollup is updated to the new type.

#+begin_src bash
rollup_addr="$(cat /opt/output/combined.json | jq -r '.rollupAddress')"

cat tools/updateRollup/updateRollup.json.example |
    jq --arg rum $rollup_manager_addr \
       --arg sk $admin_private_key \
       --arg ru $rollup_addr \
       --arg tld 60 '
           .polygonRollupManagerAddress = $rum |
           .timelockDelay = $tld |
           .deployerPvtKey = $sk |
           .newRollupTypeID = 2 |
           .rollupAddress = $ru' > tools/updateRollup/updateRollup.json

npx hardhat run ./tools/updateRollup/updateRollup.ts --network localhost
#+end_src

This looks to have been executed successfully on chain as well:

https://sepolia.etherscan.io/tx/0xf51e63e779a643091a3d133d12f84b333c6d0e2c48033243e5a14d2a5e796922

In my very specific case, I'm going to need to update the trusted
sequencer URL because it's going to change from
~zkevm-node-sequencer-001~ to ~cdk-erigon-sequencer-001~.

#+begin_src bash
rollup_addr="$(cat /opt/output/combined.json | jq -r '.rollupAddress')"

cast send --private-key "$private_key" --rpc-url "$rpc_url" "$rollup_addr" 'setTrustedSequencerURL(string)' http://cdk-erigon-sequencer-001:8123
#+end_src

Now the trusted sequencer url should be updated to work with Erigon
rather than the legacy node. If the old infra is still running, this
should stop it.

https://sepolia.etherscan.io/tx/0x1f6de118818fc1858f9ed0150e4aa240704f41e52510d844a898f2c5224a9653

All of the commands that we need to execute inside this container are
done! You can close that shell now. We should also be completed
prepared at this point to shut down the old infra. In my case, I'm
going to clean up everything:

#+begin_src bash
kurtosis clean --all
#+end_src

** Deploying the New Infra

Back to Kurtosis, we'll need to make some modifications to the yaml
file. I'm going to run this command from the root of the Kurtosis CDK
repo.

#+begin_src yaml
# this is the directory we made earlier
work_dir="/tmp/tmp.qymc4EBiyK"

yq -y '
  .args.zkevm_contracts_image = "leovct/zkevm-contracts:v8.0.0-rc.4-fork.12" |
  .args.zkevm_prover_image = "hermeznetwork/zkevm-prover:v8.0.0-RC14-fork.12" |
  .args.cdk_erigon_node_image = "hermeznetwork/cdk-erigon:v2.60.0-beta10" |
  .args.zkevm_da_image = "0xpolygon/cdk-data-availability:0.0.11" |
  .args.zkevm_bridge_service_image = "hermeznetwork/zkevm-bridge-service:v0.6.0-RC7" |
  .args.consensus_contract_type = "cdk_validium" |
  .args.sequencer_type = "erigon" |
  .deployment_stages.deploy_cdk_erigon_node = true |
  .args.use_previously_deployed_contracts = true |
  .args.erigon_datadir_archive = "../templates/contract-deploy/erigon-data" |
  .args.chain_name = "migrationexample"
' sepolia-test.yml > sepolia-erigon-test.yml

# This is the temp directory that we made earlier
rm -rf templates/contract-deploy/genesis.json; cp $work_dir/genesis.json templates/contract-deploy/genesis.json
rm -rf templates/contract-deploy/combined.json; cp $work_dir/combined.json templates/contract-deploy/combined.json
rm -rf templates/contract-deploy/conf; cp $work_dir/conf/* templates/contract-deploy/
rm -rf templates/contract-deploy/erigon-data; cp -r $work_dir/data templates/contract-deploy/erigon-data

# try to spin up the new network
kurtosis run --enclave cdk --args-file sepolia-erigon-test.yml .
#+end_src

At this point all of the new infra should be provision and ideally
following the chain that we provisioned earlier. We can do a few quick
sanity checks:

#+begin_src bash
# Make sure the logs of the important services
kurtosis service logs cdk cdk-erigon-sequencer-001
kurtosis service logs cdk cdk-erigon-rpc-001
kurtosis service logs cdk cdk-node-001

# Try to send another tx
cast send --legacy --private-key "0x163d6d315db71700ad91eae84439a4cd1afa6f80372a84859070672c0a8bca04" --rpc-url $(kurtosis port print cdk cdk-erigon-sequencer-001 rpc) --value 0.01ether 0x0000000000000000000000000000000000000000
cast send --legacy --private-key "0x163d6d315db71700ad91eae84439a4cd1afa6f80372a84859070672c0a8bca04" --rpc-url $(kurtosis port print cdk cdk-erigon-rpc-001 rpc) --value 0.01ether 0x0000000000000000000000000000000000000000

# Check these numbers to make sure they've gone past 100 now
cast rpc --rpc-url $(kurtosis port print cdk cdk-erigon-sequencer-001 rpc) zkevm_batchNumber
cast rpc --rpc-url $(kurtosis port print cdk cdk-erigon-sequencer-001 rpc) zkevm_virtualBatchNumber
cast rpc --rpc-url $(kurtosis port print cdk cdk-erigon-sequencer-001 rpc) zkevm_verifiedBatchNumber
#+end_src

At this point the sequencer is running and after pointing the cdk node
to the sequencer. We've seen a batch get sequenced and finalized on
L2. Her are links to the interesting transactions:

- [[https://sepolia.etherscan.io/tx/0xd6a6ee2c82eec970ea88b9743012b5513a8901f9b417d4b0dae09c762092b7cb#eventlog][Rollup manager update]]
- [[https://sepolia.etherscan.io/tx/0x2e2996f6fd4ec9152d26db6a8cce9771cbaf8cc0a2a71f857cec935745bf0029][Add rollup type]]
- [[https://sepolia.etherscan.io/tx/0xf51e63e779a643091a3d133d12f84b333c6d0e2c48033243e5a14d2a5e796922][Update rollup]]
- [[https://sepolia.etherscan.io/tx/0x1f6de118818fc1858f9ed0150e4aa240704f41e52510d844a898f2c5224a9653#eventlog][Updating the trusted sequencer URL]]
- [[https://sepolia.etherscan.io/tx/0xb14ad1c69a9d52ed9a108f88876c6cbdd0335d2c6cf8a392764451006ef69223#eventlog][First sequenced batch after upgrade]]
- [[https://sepolia.etherscan.io/tx/0xef87fe6e709ea7f4fad1f9c9dfcd79836f53192679b6d0602a33c0c79acd3370][First verified batch after upgrade]]
- [[https://sepolia.etherscan.io/tx/0x8d55277613700157ad643386610887bc57d7623524ef053c5420950499baa555][First bridge deposit]]



* TODO Understand missing acc input hash errors
* TODO Resolve this error

#+begin_example
[cdk-node-001] 2024-11-19T02:14:45.117Z	INFO	aggregator/aggregator.go:1328	Sending zki + batch to the prover, batchNumber [124]	{"pid": 9, "version": "v0.4.0-beta10", "module": "aggregator", "prover": "test-prover", "proverId": "dfd13057-9e5f-4fd3-a17b-58834a198b5e", "proverAddr": "172.16.0.15:48372", "batch": 124}
[cdk-node-001] 2024-11-19T02:14:45.117Z	ERROR	aggregator/aggregator.go:1490	Error getting l1InfoTreeLeaf: not found	{"pid": 9, "version": "v0.4.0-beta10", "module": "aggregator"}
[cdk-node-001] github.com/0xPolygon/cdk/aggregator.(*Aggregator).buildInputProver
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:1490
[cdk-node-001] github.com/0xPolygon/cdk/aggregator.(*Aggregator).tryGenerateBatchProof
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:1329
[cdk-node-001] github.com/0xPolygon/cdk/aggregator.(*Aggregator).Channel
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:453
[cdk-node-001] github.com/0xPolygon/cdk/aggregator/prover._AggregatorService_Channel_Handler
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/prover/aggregator_grpc.pb.go:109
[cdk-node-001] google.golang.org/grpc.(*Server).processStreamingRPC
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1673
[cdk-node-001] google.golang.org/grpc.(*Server).handleStream
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1794
[cdk-node-001] google.golang.org/grpc.(*Server).serveStreams.func2.1
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1029
[cdk-node-001] 2024-11-19T02:14:45.117Z	ERROR	aggregator/aggregator.go:1332	Failed to build input prover, not found	{"pid": 9, "version": "v0.4.0-beta10", "module": "aggregator", "prover": "test-prover", "proverId": "dfd13057-9e5f-4fd3-a17b-58834a198b5e", "proverAddr": "172.16.0.15:48372", "batch": 124}
[cdk-node-001] github.com/0xPolygon/cdk/aggregator.(*Aggregator).tryGenerateBatchProof
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:1332
[cdk-node-001] github.com/0xPolygon/cdk/aggregator.(*Aggregator).Channel
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:453
[cdk-node-001] github.com/0xPolygon/cdk/aggregator/prover._AggregatorService_Channel_Handler
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/prover/aggregator_grpc.pb.go:109
[cdk-node-001] google.golang.org/grpc.(*Server).processStreamingRPC
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1673
[cdk-node-001] google.golang.org/grpc.(*Server).handleStream
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1794
[cdk-node-001] google.golang.org/grpc.(*Server).serveStreams.func2.1
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1029
[cdk-node-001] 2024-11-19T02:14:45.118Z	ERROR	aggregator/aggregator.go:455	Error trying to generate proof: failed to build input prover, not found	{"pid": 9, "version": "v0.4.0-beta10", "module": "prover", "prover": "test-prover", "proverId": "dfd13057-9e5f-4fd3-a17b-58834a198b5e", "proverAddr": "172.16.0.15:48372"}
[cdk-node-001] github.com/0xPolygon/cdk/aggregator.(*Aggregator).Channel
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/aggregator.go:455
[cdk-node-001] github.com/0xPolygon/cdk/aggregator/prover._AggregatorService_Channel_Handler
[cdk-node-001] 	/go/src/github.com/0xPolygon/cdk/aggregator/prover/aggregator_grpc.pb.go:109
[cdk-node-001] google.golang.org/grpc.(*Server).processStreamingRPC
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1673
[cdk-node-001] google.golang.org/grpc.(*Server).handleStream
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1794
[cdk-node-001] google.golang.org/grpc.(*Server).serveStreams.func2.1
[cdk-node-001] 	/go/pkg/mod/google.golang.org/grpc@v1.64.0/server.go:1029
#+end_example

To try to get this unstuck, we'll try to do some bridge deposits.

#+begin_src bash
rpc_url="$(kurtosis port print cdk cdk-erigon-sequencer-001 rpc)"
private_key="0x163d6d315db71700ad91eae84439a4cd1afa6f80372a84859070672c0a8bca04"
cast send --legacy --rpc-url "$rpc_url" --private-key "$private_key" --value 100ether "0x278C89e074B9227E2aaC61FEa7DFf5c1356A419e"
cast send --legacy --rpc-url "$rpc_url" --private-key "$private_key" --value 100ether "0x635243A11B41072264Df6c9186e3f473402F94e9"
cast send --legacy --rpc-url "$rpc_url" --private-key "$private_key" --value 100ether "0xfa291C5f54E4669aF59c6cE1447Dc0b3371EF046"
cast send --legacy --rpc-url "$rpc_url" --private-key "$private_key" --value 100ether "0x619e4F055B87cB61Ef728423995166B4Cdb876C3"

l1_rpc_url="http://172.19.0.2:8545"
l2_rpc_url="$(kurtosis port print cdk cdk-erigon-rpc-001 rpc)"

lxly_bridge_addr="0xa49CA87527D0c3aB52F94dE4fa96d8F614CDB962"

private_key="0x163d6d315db71700ad91eae84439a4cd1afa6f80372a84859070672c0a8bca04"
eth_address="$(cast wallet address --private-key $private_key)"

bridge_sig="bridgeAsset(uint32,address,uint256,address,bool,bytes)"

for i in {1..30}; do
cast send \
    --rpc-url "$l1_rpc_url" \
    --value 100 \
    --private-key "$private_key" \
    "$lxly_bridge_addr" \
    "$bridge_sig" \
    $i "$eth_address" 100 "$(cast az)" "true" "0x"
done

# Use this call to check the status of your bridge... Once it's claimed on l2, you should be good to bridge the other direction
curl -s "$(kurtosis port print cdk zkevm-bridge-service-001 rpc)/bridges/$eth_address" | jq '.'

cast send --legacy \
    --rpc-url "$l2_rpc_url" \
    --value 1 \
    --private-key "$private_key" \
    "$lxly_bridge_addr" \
    "$bridge_sig" \
    0 "$eth_address" "1" "$(cast az)" "true" "0x"
#+end_src
