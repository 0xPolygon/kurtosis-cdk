# Snapshot Feature Implementation Plan

## Overview

This document outlines the step-by-step plan to implement a feature that creates a simplified snapshot of a Kurtosis environment that can run on docker-compose. The snapshot will include:
- L1 blockchain state (geth + beacon node) embedded in Docker images
- Multiple L2 networks (one of each sequencer type × consensus type combination)
- Configuration files for L2 components, AggKit, and Agglayer
- A docker-compose file to orchestrate everything

## Architecture

The solution will:
1. **Reuse existing Kurtosis logic** - Leverage `main.star` and existing modules for L1 deployment, contract deployment, and network registration
2. **Integrate via snapshot_mode flag** - Add a `snapshot_mode` flag to args that triggers snapshot logic in `main.star`
3. **L1 only runs in Kurtosis** - Only L1 services (geth + lighthouse) actually run in Kurtosis to generate state
4. **Config-only generation for L2/Agglayer/AggKit** - L2 components, Agglayer, and AggKit do NOT run in Kurtosis. Only their config files are generated using existing config generation functions
5. **Stop L1 services gracefully** - Stop geth and lighthouse at consistent state
6. **Package L1 state** - Extract state and build Docker images
7. **Generate docker-compose** - Create static docker-compose configuration where L2 services start fresh (no state capture)

## Key Integration Points

- **main.star**: Add snapshot_mode flag check, call snapshot module after contracts/configs are ready
- **Existing modules to reuse**:
  - `l1_launcher.launch()` - L1 deployment (services run to generate state)
  - `agglayer_contracts_package.run()` - Contract deployment
  - `sovereign_contracts_package.run()` - Network registration
  - Config artifact generation functions - Extract configs WITHOUT starting services:
    - `agglayer.star.create_agglayer_config_artifact()` - Agglayer config (no service needed)
    - `cdk_node.star` config generation - CDK-Node config (no service needed)
    - `aggkit.star` config generation - AggKit config (no service needed)
    - `zkevm_bridge_service.star` config generation - Bridge config (no service needed)
    - OP Stack config generation - OP Stack configs (may need refactoring to extract without services)

## Understanding the Codebase

### How Multiple Networks Work

The repo supports multiple networks via `deployment_suffix` (e.g., `-002`, `-003`). Each network requires:
- Unique `deployment_suffix` (e.g., `""`, `-002`, `-003`)
- Unique `l2_chain_id` (e.g., `20201`, `20202`, `20203`)
- Unique `network_id` (e.g., `1`, `2`, `3`)
- Unique addresses/keys for sequencer, aggregator, admin, dac, claimsponsor, sovereignadmin
- Each network is registered in a separate Kurtosis run with `deploy_l1: false` and `deploy_agglayer: false` after the first run

### L1 Volume and State Management

- L1 uses the `ethereum-package` which manages volumes internally
- Geth datadir is stored in the geth container (managed by ethereum-package)
- Lighthouse datadir is stored in the lighthouse container (managed by ethereum-package)
- These are NOT accessible via Kurtosis persistent volumes - must extract from containers directly
- Service names follow pattern: `el-1-geth-lighthouse`, `cl-1-lighthouse-geth`, `vc-1-lighthouse-geth`

### L2 Volume and State Management

- **Important**: L2 services, Agglayer, and AggKit do NOT run in Kurtosis and do NOT capture state
- **Config-only approach**: Only config files are generated in Kurtosis
- **Keystores**: Generated during contract deployment, stored in `keystores-artifact` (each network has unique keystore files)
- **Output artifacts**: Shared persistent volume `output-artifact` (contains genesis, chain configs, etc.)
- **Docker-compose**: L2 services will start fresh with empty volumes and sync from L1 on first run

### Config Generation Flow

1. **L1 Configs**: Generated by ethereum-package, not directly accessible
2. **Contract Deployment**: Creates artifacts in `output-artifact`:
   - `genesis.json` (for CDK-Erigon)
   - `dynamic-{chain_name}-conf.json` (CDK-Erigon chain config)
   - `dynamic-{chain_name}-allocs.json` (CDK-Erigon chain allocs)
   - `first-batch-config.json` (CDK-Erigon first batch)
   - Contract addresses in `output-artifact`
3. **L2 Configs**: Generated via `plan.render_templates()` creating artifacts:
   - Agglayer config (via `create_agglayer_config_artifact()`)
   - CDK-Node config (via `cdk_node.star`)
   - AggKit config (via `aggkit.star._deploy_main_aggkit_service()`)
   - Bridge config (via `zkevm_bridge_service.star`)
   - OP Stack configs (via `optimism_package.run()` for op-geth)
4. **Keystores**: Generated during contract deployment, stored in `keystores-artifact`

### Critical Dependencies

- **Genesis artifacts**: Must be generated for each network (created during contract deployment)
- **Chain configs**: CDK-Erigon requires `cdk-erigon-chain-config`, `cdk-erigon-chain-allocs`, `cdk-erigon-chain-first-batch` artifacts
- **Agglayer config**: Must be updated incrementally as each network is added (contains `[full-node-rpcs]` and `[proof-signers]` sections)
- **OP Stack configs**: For op-geth networks, OP Stack generates configs via `optimism_package.run()`

## Existing Code Reuse Strategy

### Functions/Modules to Reuse Directly

1. **L1 Deployment** (`src/l1/launcher.star`):
   - `l1_launcher.launch(plan, args)` - Deploys L1 with geth and lighthouse
   - No changes needed - works as-is for snapshot mode
   - **Note**: L1 services must run to generate state, but we stop them before extraction

2. **Contract Deployment** (`src/contracts/agglayer.star`):
   - `agglayer_contracts_package.run(plan, args, deployment_stages, op_stack_args)` - Deploys all L1 contracts
   - Generates keystores and output artifacts
   - No changes needed - works as-is

3. **Network Registration** (`src/contracts/sovereign.star`):
   - `sovereign_contracts_package.run(plan, args, predeployed_contracts)` - Creates rollup type and rollup
   - `sovereign_contracts_package.init_rollup(plan, args, deployment_stages)` - Initializes rollup
   - **Important**: Must be called for each network with unique args
   - For snapshot mode, we'll call this multiple times in a single run

4. **Config Generation** (multiple modules):
   - `agglayer.star.create_agglayer_config_artifact()` - Creates agglayer config artifact
   - `aggkit.star._deploy_main_aggkit_service()` - Creates aggkit config artifact (we extract before service starts)
   - `cdk_node.star` config generation - Creates cdk-node config artifact
   - `zkevm_bridge_service.star` - Creates bridge config artifact
   - **Key insight**: These functions create artifacts via `plan.render_templates()` which we can extract using `plan.get_files_artifact()`

5. **OP Stack Deployment** (for OP-Geth networks):
   - `optimism_package.run(plan, op_stack_args)` - Deploys OP Stack infrastructure
   - Used when `sequencer_type == "op-geth"`
   - Generates configs we can extract
   - **Note**: This may start services, but we can stop them after config extraction

6. **Chain Launcher** (`src/chain/launcher.star`):
   - `chain_launcher.launch()` - Currently starts services AND generates configs
   - **Issue**: Configs are generated as part of service deployment
   - **Solution for snapshot mode**: Extract config generation logic without starting services
   - **Approach**: 
     - For CDK-Erigon: Configs are generated via `plan.render_templates()` - can extract without services
     - For OP-Geth: May need to refactor `optimism_package.run()` to support config-only mode, OR extract configs after minimal service startup (TBD)
   - **Decision**: In snapshot mode, we extract configs WITHOUT starting L2 services. Services will run fresh in docker-compose.

### Files to Modify (Minimal Changes)

1. **`src/package_io/input_parser.star`**:
   - Add `snapshot_mode: False` to default args
   - Add `snapshot_output_dir: ""` parameter
   - Add `snapshot_networks: []` parameter (list of network configs to create)
   - **Change**: ~10 lines

2. **`main.star`**:
   - Import snapshot module: `snapshot_package = import_module("./src/snapshot/snapshot.star")`
   - Check `snapshot_mode` flag after parsing args
   - When `snapshot_mode=True`:
     - Deploy L1 and contracts (first network only)
     - For each network in `snapshot_networks`:
       - Register network (sovereign contracts)
       - Generate configs (start services briefly, extract configs, stop services)
     - Call `snapshot_package.run()` to extract all artifacts and prepare for state extraction
   - **Change**: ~50-70 lines

3. **`src/chain/launcher.star`** (if needed):
   - May need to add a flag to stop services after config generation in snapshot mode
   - Or ensure we can stop services gracefully after config extraction
   - **Change**: TBD after analysis, likely minimal

### New Code Structure

- **`src/snapshot/snapshot.star`**: Main orchestration, calls existing functions
- **`src/snapshot/config_extractor.star`**: Extracts artifacts using `plan.get_files_artifact()`
- **`src/snapshot/state_extractor.star`**: Prepares L1 for state extraction
- **`src/snapshot/network_registrar.star`**: Handles multiple network registration
- **`snapshot/scripts/*.sh`**: Post-processing scripts (Docker operations, file processing)

## Directory Structure

New code will be placed in two locations:
1. **Starlark code** (integrated with existing codebase): `src/snapshot/`
2. **Shell scripts and templates** (isolated): `snapshot/`

```
src/snapshot/
├── snapshot.star                 # Main snapshot orchestration module
├── config_extractor.star         # Extract config artifacts from plan
├── state_extractor.star          # Extract L1 state from services
├── network_registrar.star        # Register multiple networks
└── compose_generator.star        # Generate docker-compose.yml

snapshot/
├── scripts/
│   ├── snapshot.sh               # User-facing entry point (calls kurtosis with snapshot_mode)
│   ├── extract-l1-state.sh      # Extract L1 state after services stopped
│   ├── build-l1-images.sh       # Build Docker images with L1 state
│   ├── process-configs.sh       # Process extracted configs to static format
│   └── generate-compose.sh       # Generate docker-compose.yml
├── templates/
│   ├── docker-compose.template.yml
│   ├── geth.Dockerfile.template
│   └── lighthouse.Dockerfile.template
├── utils/
│   ├── kurtosis-helpers.sh       # Helper functions for Kurtosis operations
│   ├── config-processor.sh       # Process dynamic configs to static
│   └── state-extractor.sh        # Extract state from containers
└── README.md                     # Documentation
```

## Implementation Steps

### Step 1: Create Directory Structure and Snapshot Module Skeleton

**Objective**: Set up the snapshot directory structure and create the Starlark module skeleton.

**Tasks**:
1. Create `src/snapshot/` directory for Starlark modules
2. Create `snapshot/` directory with subdirectories: `scripts/`, `templates/`, `utils/`
3. Create `src/snapshot/snapshot.star` with basic structure:
   - `run(plan, args, deployment_stages, ...)` function signature
   - Placeholder for orchestration logic
4. Create `src/snapshot/network_registrar.star`:
   - `register_networks(plan, args, contract_setup_addresses, ...)` function
   - Handles multiple network registration in a single run
5. Create `snapshot/utils/kurtosis-helpers.sh` with helper functions:
   - `get_service_uuid(service_name)` - Get container UUID from service name
   - `stop_service(enclave, service)` - Stop a Kurtosis service
   - `extract_file(container, src, dest)` - Extract file from container
   - `wait_for_service(enclave, service, condition)` - Wait for service state
6. Create `snapshot/README.md` with basic documentation structure

**Acceptance Criteria**:
- All directories exist in both `src/snapshot/` and `snapshot/`
- `snapshot.star` module can be imported and has `run()` function
- Helper functions are properly structured and can be called
- README has basic structure with placeholder sections

---

### Step 2: Integrate Snapshot Mode into main.star

**Objective**: Add snapshot_mode flag support to main.star to trigger snapshot logic at the right time.

**Tasks**:
1. Modify `src/package_io/input_parser.star`:
   - Add `snapshot_mode` to default args (default: False)
   - Add `snapshot_output_dir` parameter for output location
   - Add `snapshot_networks` parameter (list of network configs):
     ```python
     snapshot_networks: [
       {
         "sequencer_type": "cdk-erigon",
         "consensus_type": "rollup",
         "deployment_suffix": "-001",
         "l2_chain_id": 20201,
         "network_id": 1,
         # ... other network-specific args
       },
       # ... more networks
     ]
     ```
2. Modify `main.star`:
   - Check for `snapshot_mode` flag after parsing args
   - When `snapshot_mode` is True:
     - Deploy L1 (only if first run, reuse for subsequent networks)
     - Deploy agglayer contracts (only if first network)
     - For each network in `snapshot_networks`:
       - Set args with network-specific values
       - Register network (sovereign contracts)
       - Generate configs WITHOUT starting services:
         - Extract config artifacts using existing config generation functions
         - Do NOT start L2 services, Agglayer, or AggKit
         - Configs are generated via `plan.render_templates()` - no services needed
       - Update agglayer config with new network
     - After all networks are registered and configs generated, call snapshot module
     - Pass all necessary context (plan, args, contract_setup_addresses, networks_metadata)
3. Import snapshot module in `main.star`: `snapshot_package = import_module("./src/snapshot/snapshot.star")`

**Acceptance Criteria**:
- `snapshot_mode` flag is recognized in args
- `snapshot_networks` parameter is parsed correctly
- When `snapshot_mode=True`, multiple networks are registered
- Config artifacts are generated for each network WITHOUT starting L2/Agglayer/AggKit services
- L1 services run (to generate state) but L2 services do NOT run
- Snapshot module is called at the correct point in the flow
- Existing functionality is unchanged when `snapshot_mode=False`

---

### Step 3: Implement Multi-Network Registration in Snapshot Mode

**Objective**: Reuse existing network registration logic to add multiple networks in a single Kurtosis run.

**Tasks**:
1. Analyze existing network registration flow:
   - `sovereign_contracts_package.run()` - Creates rollup type and rollup
   - `chain_launcher.launch()` - Generates configs and starts services
   - Contract deployment scripts in `static_files/contracts/`
2. Create `src/snapshot/network_registrar.star`:
   - `register_networks(plan, args, contract_setup_addresses, snapshot_networks, ...)` function
   - For each network in `snapshot_networks`:
     - Create network-specific args (merge base args with network config)
     - Reuse `sovereign_contracts_package.run()` with network-specific args
     - Reuse `sovereign_contracts_package.init_rollup()` with network-specific args
     - For CDK-Erigon networks:
       - Extract config generation logic from `chain_launcher.launch()` without starting services
       - Generate configs via `plan.render_templates()` (cdk-node, aggkit, bridge configs)
       - Extract config artifacts (no services started)
     - For OP-Geth networks:
       - Extract config generation logic from `optimism_package.run()` without starting services
       - OR: Start services minimally, extract configs, then stop (if config-only mode not available)
       - Extract config artifacts
     - Collect network metadata (rollup ID, chain ID, addresses, service names)
   - Store network metadata in plan for later extraction
   - Return aggregated network metadata
3. Integrate into `src/snapshot/snapshot.star`:
   - Call `register_networks()` from snapshot `run()` function
   - Handle agglayer config updates incrementally

**Acceptance Criteria**:
- Multiple networks are registered in a single Kurtosis run
- Existing `sovereign_contracts_package.run()` logic is reused
- Each network gets unique `deployment_suffix`, `network_id`, `chain_id`
- Network metadata is collected and stored
- Configs are generated WITHOUT starting L2/Agglayer/AggKit services
- Agglayer config is updated with each network

---

### Step 4: Implement Config Artifact Extraction

**Objective**: Extract configuration artifacts that were already generated by existing code, before services are stopped.

**Tasks**:
1. Analyze existing config artifact creation:
   - `agglayer.star.create_agglayer_config_artifact()` - Creates agglayer config artifact
   - `aggkit.star._deploy_main_aggkit_service()` - Creates aggkit config artifact
   - `cdk_node.star` - Creates cdk-node config artifact
   - `zkevm_bridge_service.star` - Creates bridge config artifact
   - Contract deployment creates: genesis.json, chain configs, allocs, first-batch-config
   - Keystores are stored in `keystores-artifact` persistent volume
2. Create `src/snapshot/config_extractor.star`:
   - `extract_config_artifacts(plan, args, networks_metadata)` function
   - For each registered network:
     - Extract agglayer config artifact (updated with all networks)
     - Extract network-specific configs:
       - CDK-Erigon: cdk-node config, genesis, chain configs, allocs, first-batch-config
       - OP-Geth: op-node config, op-geth config, op-proposer config (if applicable)
     - Extract keystore artifacts:
       - Use `plan.store_service_files()` to extract from contracts service
       - Extract sequencer, aggregator, claimsponsor keystores
     - Extract genesis and chain config artifacts:
       - Use `plan.get_files_artifact()` or `plan.store_service_files()`
       - Extract from `output-artifact` persistent volume
     - Store artifacts in a snapshot-specific location using `plan.store_service_files()`
   - Create a manifest of all extracted artifacts with metadata
3. Integrate into `src/snapshot/snapshot.star`:
   - Call config extraction after networks are registered
   - Store artifact references for later processing

**Acceptance Criteria**:
- Config artifacts are extracted using existing Kurtosis APIs
- All config artifacts for each network are captured
- Keystore artifacts are extracted for each network
- Genesis and chain config artifacts are extracted
- Artifact manifest is created with metadata
- Agglayer config includes all registered networks

---

### Step 5: Implement L1 State Extraction

**Objective**: Stop L1 services gracefully and extract their state using Kurtosis APIs, then export via shell script.

**Tasks**:
1. Create `src/snapshot/state_extractor.star`:
   - `prepare_l1_snapshot(plan, args, l1_context)` function
   - Wait for L1 to reach finalized state (check via L1 RPC):
     - Wait for finalized block (check `eth_getBlockByNumber("finalized")`)
     - Wait for finalized slot (check beacon API)
   - Use `plan.exec()` to stop geth and lighthouse services gracefully:
     - Stop geth: `plan.exec(service_name="el-1-geth-lighthouse", command=["pkill", "-SIGTERM", "geth"])`
     - Wait for geth to stop cleanly
     - Stop lighthouse: `plan.exec(service_name="cl-1-lighthouse-geth", command=["pkill", "-SIGTERM", "lighthouse"])`
     - Wait for lighthouse to stop cleanly
   - Store L1 service names and paths for later extraction:
     - Geth datadir: typically `/root/.ethereum` or `/data` (check ethereum-package)
     - Lighthouse datadir: typically `/root/.lighthouse` or `/data` (check ethereum-package)
   - Create a snapshot service that will extract state after services are stopped
2. Create `snapshot/scripts/extract-l1-state.sh`:
   - Called after Kurtosis run completes (in snapshot mode)
   - Uses `kurtosis service exec` or `docker cp` to extract:
     - Geth datadir from geth service container:
       - Service name: `el-1-geth-lighthouse`
       - Extract entire datadir directory
     - Lighthouse datadir from lighthouse service container:
       - Service name: `cl-1-lighthouse-geth`
       - Extract entire datadir directory
   - Verifies state consistency:
     - Check geth finalized block number
     - Check lighthouse finalized slot
     - Verify they correspond to the same state
   - Extracts to `output-dir/l1-state/geth/` and `output-dir/l1-state/lighthouse/`
   - Creates state manifest with block number, slot, chain ID, datadir paths
3. Create `snapshot/utils/state-extractor.sh` helper functions:
   - `wait_for_finalized_block(enclave, l1_rpc_url, min_blocks)` - Wait for finalized state
   - `stop_service_gracefully(enclave, service)` - Stop service cleanly
   - `extract_datadir(container, src_path, dest_path)` - Extract state directory
   - `verify_state_consistency(geth_block, lighthouse_slot)` - Verify consistency
   - `get_l1_service_names(enclave)` - Get actual L1 service names from enclave

**Acceptance Criteria**:
- L1 services are stopped gracefully via Kurtosis APIs
- State extraction happens after Kurtosis run completes
- Geth and lighthouse state is extracted successfully
- State consistency is verified (same finalized block/slot)
- State manifest is created with all necessary metadata
- Extracted state structure is correct for Docker image building
- Service names are correctly identified from the enclave

---

### Step 6: Process Extracted Configs to Static Format

**Objective**: Convert extracted config artifacts from dynamic Kurtosis format to static docker-compose format.

**Tasks**:
1. Create `snapshot/utils/config-processor.sh` with functions to:
   - Read config files (TOML, JSON, YAML)
   - Replace template variables with static values
   - Convert Kurtosis service names to docker-compose service names:
     - Pattern: `{service}{deployment_suffix}` → `{service}-{network_id}`
     - Example: `cdk-node-002` → `cdk-node-2`
   - Replace dynamic ports with static ports from allocation scheme:
     - Use port allocation from `args["static_ports"]` if available
     - Otherwise, use default port ranges per service type
   - Update service URLs to use docker-compose service names:
     - `http://cdk-erigon-rpc-002:8123` → `http://cdk-erigon-rpc-2:8123`
   - Preserve file structure and comments
2. Create `snapshot/scripts/process-configs.sh`:
   - Reads artifact manifest from Kurtosis run
   - For each extracted config artifact:
     - Copies artifact files to processing directory
     - Processes configs using config-processor functions:
       - Agglayer config: Update `[full-node-rpcs]` and `[proof-signers]` sections
       - CDK-Node config: Update RPC URLs, database URLs, agglayer endpoint
       - AggKit config: Update RPC URLs, database URLs
       - Bridge config: Update RPC URLs, database URLs
       - OP Stack configs: Update service URLs
     - Stores processed configs in `output-dir/configs/<network-id>/`
   - Creates port allocation mapping for docker-compose
   - Updates agglayer config to include all networks with static references
   - Generates keystore mapping (which keystore belongs to which network)
3. Integrate into snapshot flow:
   - Called after Kurtosis run completes
   - Processes all extracted config artifacts

**Acceptance Criteria**:
- All config files are processed to static format
- Template variables are replaced with actual values
- Service names match docker-compose service names
- Ports are allocated statically without conflicts
- Config files are syntactically valid after processing
- Original structure and formatting is preserved
- Agglayer config includes all networks correctly

---

### Step 7: Create Docker Image Build Scripts

**Objective**: Create Dockerfiles and build scripts to package L1 state into Docker images.

**Tasks**:
1. Research ethereum-package to determine exact datadir paths:
   - Check ethereum-package documentation or source
   - Verify geth datadir path (likely `/root/.ethereum` or `/data`)
   - Verify lighthouse datadir path (likely `/root/.lighthouse` or `/data`)
2. Create `snapshot/templates/geth.Dockerfile.template`:
   - Base image: official geth image (use same version as deployed)
   - Copy extracted geth datadir to appropriate location
   - Set entrypoint to run geth with the state:
     - Use `--datadir` flag pointing to copied datadir
     - Configure to listen on standard ports (8545, 8546)
     - Use `--http.addr 0.0.0.0` and `--ws.addr 0.0.0.0`
     - Include other necessary flags from original deployment
   - Configure to serve the correct chain ID
3. Create `snapshot/templates/lighthouse.Dockerfile.template`:
   - Base image: official lighthouse image (use same version as deployed)
   - Copy extracted lighthouse datadir to appropriate location
   - Set entrypoint to run lighthouse with the state:
     - Use `--datadir` flag pointing to copied datadir
     - Configure to connect to geth (use docker-compose service name)
     - Include other necessary flags from original deployment
4. Create `snapshot/scripts/build-l1-images.sh` that:
   - Processes Dockerfile templates with actual paths
   - Determines geth and lighthouse image versions from snapshot metadata
   - Builds `l1-geth:snapshot` image
   - Builds `l1-lighthouse:snapshot` image
   - Tags images appropriately
   - Verifies images are built successfully
   - Creates image manifest with versions and paths

**Acceptance Criteria**:
- Dockerfiles are generated correctly from templates
- Images build successfully without errors
- Images contain the extracted state data
- Images can be started with `docker run` and connect to each other
- Geth serves the correct chain ID and block number
- Lighthouse connects to geth successfully
- Image versions match deployed versions

---

### Step 8: Generate Docker Compose Configuration

**Objective**: Generate a complete docker-compose.yml file that orchestrates all components.

**Tasks**:
1. Create `snapshot/templates/docker-compose.template.yml` with:
   - L1 services:
     - `l1-geth`: Uses `l1-geth:snapshot` image, mounts geth datadir
     - `l1-lighthouse`: Uses `l1-lighthouse:snapshot` image, mounts lighthouse datadir, depends on l1-geth
   - Agglayer service:
     - Uses agglayer image from args
     - Mounts processed agglayer config
     - Mounts aggregator keystore
     - Depends on L1 services
     - **No persistent volumes** - starts fresh
   - For each network:
     - L2 execution client:
       - CDK-Erigon: `cdk-erigon-sequencer-{id}`, `cdk-erigon-rpc-{id}`
       - OP-Geth: `op-geth-{id}`, `op-node-{id}` (if applicable)
     - L2 consensus components:
       - CDK-Node: `cdk-node-{id}` (for CDK-Erigon)
       - OP-Proposer: `op-proposer-{id}` (for OP-Geth, if applicable)
     - AggKit service: `aggkit-{id}`
     - Bridge service: `zkevm-bridge-{id}` (if applicable)
     - ZKEVM Prover: `zkevm-prover-{id}` (if applicable)
     - Pool Manager: `zkevm-pool-manager-{id}` (for CDK-Erigon)
     - Data Availability: `zkevm-dac-{id}` (for Validium)
   - Networks configuration:
     - Create Docker network for services to communicate
   - Volume mounts:
     - Config files for each service
     - Keystores for each service
     - **Empty data directories** for L2 services (they sync from L1 on first run)
   - Environment variables:
     - Set appropriate env vars for each service
   - Service dependencies:
     - L1 before L2
     - L2 execution before L2 consensus
     - Agglayer before L2 components
2. Create `snapshot/scripts/generate-compose.sh` that:
   - Reads network metadata from snapshot state
   - Reads processed configs from `output-dir/configs/`
   - Processes docker-compose template with all network data
   - Sets up proper service dependencies
   - Configures static ports (no conflicts):
     - Use port allocation from processed configs
     - Map ports: `{service_port}:{host_port}`
   - Generates volume mounts:
     - Config files: `./configs/{network-id}/{config-file}:/etc/{service}/{config-file}`
     - Keystores: `./keystores/{network-id}/{keystore-file}:/etc/{service}/{keystore-file}`
     - Empty data volumes for L2 services (named volumes, no host paths)
   - Generates final `docker-compose.yml` in output directory
3. Create helper functions in `snapshot/utils/compose-generator.sh`:
   - `generate_service_config(service_type, network_id, config)` - Generate service config
   - `generate_volume_mounts(service_type, network_id, configs)` - Generate volume mounts
   - `generate_ports(service_type, network_id, port_config)` - Generate port mappings
   - `generate_dependencies(service_type, network_id, networks)` - Generate service dependencies

**Acceptance Criteria**:
- docker-compose.yml is generated with all services
- All services have proper dependencies (L1 before L2, etc.)
- Ports are static and don't conflict
- Config files are mounted correctly
- Keystores are mounted correctly
- Environment variables are set appropriately
- Network configuration allows services to communicate
- Services reference the correct Docker images
- Volume paths are relative to docker-compose.yml location

---

### Step 9: Configure Docker Compose for Fresh L2 Services

**Objective**: Configure docker-compose so L2 services start fresh with empty volumes and sync from L1.

**Tasks**:
1. Configure docker-compose volumes for L2 services:
   - Use empty/named volumes for L2 data directories
   - CDK-Erigon: Empty volume for `cdk-erigon-datadir`
   - OP-Geth: Empty volumes for op-geth and op-node datadirs
   - AggKit: Empty volumes for `aggkit-data` and `aggkit-tmp`
   - Agglayer: No persistent volumes needed (stateless)
2. Ensure services can sync from L1:
   - Verify L2 services can connect to L1 services
   - Verify genesis files and chain configs are properly mounted
   - Verify keystores are properly mounted
3. Document that L2 services will perform initial sync on first run:
   - CDK-Erigon will sync from L1
   - OP-Geth will sync from L1
   - This is expected behavior and part of the snapshot design

**Acceptance Criteria**:
- Docker-compose uses empty volumes for all L2 services
- L2 services can successfully sync from L1 on first run
- No L2 state is captured or extracted
- Services start correctly with empty volumes

---

### Step 10: Add Error Handling and Validation

**Objective**: Add comprehensive error handling, logging, and validation throughout the process.

**Tasks**:
1. Add error handling to all scripts:
   - Check prerequisites (kurtosis CLI, docker, etc.)
   - Validate inputs (enclave name, output directory, network configs)
   - Handle Kurtosis API errors gracefully
   - Rollback on failures where possible
2. Add logging:
   - Progress indicators for each step
   - Detailed logs to `snapshot.log`
   - Error messages with actionable guidance
3. Add validation:
   - Verify L1 state consistency before snapshot
   - Validate extracted config files (syntax, required fields)
   - Check Docker images after build
   - Validate docker-compose.yml syntax
   - Verify all required files exist
4. Add network validation:
   - Verify unique deployment_suffix, chain_id, network_id
   - Verify required addresses/keys are provided
   - Verify sequencer/consensus type combinations are valid

**Acceptance Criteria**:
- Scripts fail fast with clear error messages
- All errors are logged with context
- Prerequisites are checked before starting
- Validation catches issues early
- Logs are readable and helpful for debugging
- Network configs are validated before processing

---

### Step 11: Create User-Facing Entry Point Script

**Objective**: Create a simple script that users can run to trigger snapshot creation.

**Tasks**:
1. Create `snapshot/scripts/snapshot.sh`:
   - Accepts parameters:
     - `--enclave-name` (required): Kurtosis enclave name
     - `--output-dir` (required): Output directory for snapshot
     - `--networks` (optional): JSON file with network configs (default: generate all combinations)
     - `--l1-wait-blocks` (optional): Number of blocks to wait for L1 finalization (default: 10)
   - Generates args file with `snapshot_mode: true` and network configs
   - Runs `kurtosis run` with snapshot mode enabled
   - After Kurtosis completes, calls post-processing scripts:
     - `extract-l1-state.sh`
     - `process-configs.sh`
     - `build-l1-images.sh`
     - `generate-compose.sh`
   - Validates output and reports success/failure
   - Creates summary report
2. Create wrapper at project root: `create-snapshot.sh`
   - Calls `snapshot/scripts/snapshot.sh` with proper paths
   - Provides user-friendly interface and help text
   - Validates environment (kurtosis CLI, docker, etc.)

**Acceptance Criteria**:
- Script accepts all necessary parameters
- Script runs full snapshot flow end-to-end
- Script provides clear progress output
- Script handles errors gracefully
- Wrapper script works from project root
- Help text is clear and comprehensive

---

### Step 12: Create Documentation

**Objective**: Document the snapshot feature for users.

**Tasks**:
1. Update `snapshot/README.md` with:
   - Feature overview and use cases
   - Prerequisites and installation
   - Usage instructions with examples
   - Step-by-step explanation of what the script does
   - Network configuration format
   - Troubleshooting guide
   - Limitations and known issues
2. Add inline documentation to scripts:
   - Header comments explaining purpose
   - Function documentation
   - Parameter descriptions
3. Create example network configuration files:
   - `snapshot/examples/networks-all-combinations.json`
   - `snapshot/examples/networks-cdk-erigon-only.json`
   - `snapshot/examples/networks-op-geth-only.json`
4. Create example output structure documentation

**Acceptance Criteria**:
- README is comprehensive and clear
- Usage examples work as documented
- Troubleshooting section covers common issues
- All scripts have adequate inline documentation
- Documentation explains the snapshot process clearly
- Example configs are provided and validated

---

### Step 13: Add Testing and Verification

**Objective**: Add verification steps to ensure the snapshot works correctly.

**Tasks**:
1. Create `snapshot/scripts/verify-snapshot.sh` that:
   - Validates all required files exist
   - Checks Docker images are present
   - Validates docker-compose.yml syntax
   - Optionally starts the environment and verifies:
     - L1 is accessible and at correct block
     - L2 networks are accessible
     - Agglayer is running
     - Services can communicate
     - All networks are registered in agglayer
2. Add verification to main `snapshot.sh`:
   - Run verification after snapshot creation
   - Report success/failure clearly
3. Create a test script that can be run manually to validate the entire flow

**Acceptance Criteria**:
- Verification script checks all critical components
- Snapshot validation passes for a successful run
- Verification provides clear pass/fail results
- Test script can be used to validate the feature end-to-end

---

### Step 14: Integration and Cleanup

**Objective**: Integrate the snapshot feature into the main project and clean up.

**Tasks**:
1. Update main project `README.md` to mention snapshot feature
2. Add snapshot scripts to `.gitignore` exclusions (if needed)
3. Ensure snapshot directory follows project conventions
4. Add any necessary changes to existing files (minimal):
   - Document any required parameter changes
   - Update input_parser documentation
5. Create a simple wrapper script at project root: `create-snapshot.sh`
   - Calls `snapshot/scripts/snapshot.sh` with proper paths
   - Provides user-friendly interface

**Acceptance Criteria**:
- Snapshot feature is discoverable from main README
- Wrapper script works from project root
- No breaking changes to existing functionality
- All code follows project style and conventions
- Feature is ready for use

---

## Key Design Decisions

1. **Reuse Existing Logic**: Instead of duplicating Kurtosis deployment logic, we integrate with `main.star` and reuse existing modules:
   - `l1_launcher.launch()` for L1 deployment
   - `agglayer_contracts_package.run()` for contract deployment
   - `sovereign_contracts_package.run()` for network registration
   - Existing config generation functions (which create artifacts we can extract)

2. **Snapshot Mode Integration**: Add `snapshot_mode` flag to `main.star` that:
   - Deploys L1 and contracts (first network only)
   - Registers multiple networks in a single run
   - Generates configs for each network (starts services briefly, extracts configs, stops services)
   - Calls snapshot module at the right point in the flow
   - Maintains backward compatibility (no changes when flag is False)

3. **Multiple Network Handling**: Support multiple networks via `snapshot_networks` parameter:
   - Each network gets unique `deployment_suffix`, `chain_id`, `network_id`
   - Networks are registered sequentially in a single Kurtosis run
   - Agglayer config is updated incrementally as each network is added
   - All network metadata is collected for docker-compose generation

4. **Artifact-Based Config Extraction**: Leverage Kurtosis artifact system:
   - Configs are generated as artifacts by existing code
   - Use `plan.get_files_artifact()` and `plan.store_service_files()` to retrieve artifacts
   - Extract from persistent volumes (`keystores-artifact`, `output-artifact`)
   - No need to parse containers or services for configs

5. **L1 State Extraction**: Extract L1 state from containers directly:
   - L1 volumes are managed by ethereum-package, not accessible via Kurtosis volumes
   - Extract datadirs from geth and lighthouse containers using `kurtosis service exec` or `docker cp`
   - Stop services gracefully before extraction
   - Verify state consistency before packaging

6. **Config-Only Generation for L2/Agglayer/AggKit**: 
   - L2 services, Agglayer, and AggKit do NOT run in Kurtosis
   - Only config files are generated using existing `plan.render_templates()` functions
   - Configs are generated as artifacts without starting services
   - This simplifies the snapshot process and keeps snapshots smaller

7. **Static Configuration Processing**: Convert dynamic Kurtosis configs to static docker-compose format:
   - Process artifacts after extraction
   - Replace template variables with actual values
   - Map service names and ports to docker-compose equivalents
   - Preserve original structure and formatting

8. **L2 State Handling**: 
   - **Decision**: L2 services start fresh with empty volumes in docker-compose
   - Services will sync from L1 on first run (expected behavior)
   - No L2 state is captured or extracted
   - This keeps snapshots smaller and simpler

## Dependencies

- Kurtosis CLI (existing)
- Docker and docker-compose (existing)
- jq for JSON processing
- Standard Unix tools (bash, sed, awk)
- Access to Kurtosis enclave for state extraction (after run completes)
- Understanding of ethereum-package datadir structure

## Integration Flow

1. **User runs**: `./create-snapshot.sh --enclave-name snapshot --output-dir ./snapshot-output --networks networks.json`
2. **Script generates** args file with `snapshot_mode: true` and `snapshot_networks` list
3. **Kurtosis runs** `main.star` with snapshot mode:
   - Deploys L1 (reuses `l1_launcher.launch()`)
   - Deploys contracts (reuses `agglayer_contracts_package.run()`)
   - For each network in `snapshot_networks`:
     - Registers network (reuses `sovereign_contracts_package.run()`)
     - Generates configs WITHOUT starting L2/Agglayer/AggKit services (config-only mode)
     - Updates agglayer config
   - Calls `snapshot.star.run()` to extract all config artifacts
   - Stops L1 services gracefully (only L1 services run in Kurtosis)
4. **Post-processing scripts** (after Kurtosis completes):
   - Extract L1 state from stopped services
   - Process config artifacts to static format
   - Build Docker images with L1 state
   - Generate docker-compose.yml
5. **Output**: Complete snapshot in `output-dir/` ready for docker-compose

## Network Configuration Format

Example `networks.json`:

```json
{
  "networks": [
    {
      "sequencer_type": "cdk-erigon",
      "consensus_type": "rollup",
      "deployment_suffix": "-001",
      "l2_chain_id": 20201,
      "network_id": 1,
      "l2_sequencer_address": "0x5b06837A43bdC3dD9F114558DAf4B26ed49842Ed",
      "l2_sequencer_private_key": "0x183c492d0ba156041a7f31a1b188958a7a22eebadca741a7fe64436092dc3181",
      "l2_aggregator_address": "0xCae5b68Ff783594bDe1b93cdE627c741722c4D4d",
      "l2_aggregator_private_key": "0x2857ca0e7748448f3a50469f7ffe55cde7299d5696aedd72cfe18a06fb856970",
      "l2_admin_address": "0xE34aaF64b29273B7D567FCFc40544c5B272F08ACc1",
      "l2_admin_private_key": "0x12d7de8621a77640c9241b2595ba78ce443d05e94090365ab3bb5e19df82c625",
      "l2_dac_address": "0x5951F5b2604c9B42E478d5e2B2437F44073eF9A6",
      "l2_dac_private_key": "0x85d836ee6ea6f48bae27b31535e6fc2eefe056f2276b9353aafb294277d8159b",
      "l2_claimsponsor_address": "0x635243A11B41072264Df6c9186e3f473402F94e9",
      "l2_claimsponsor_private_key": "0x986b325f6f855236b0b04582a19fe0301eeecb343d0f660c61805299dbf250eb",
      "l2_sovereignadmin_address": "0xc653eCD4AC5153a3700Fb13442Bcf00A691cca16",
      "l2_sovereignadmin_private_key": "0xa574853f4757bfdcbb59b03635324463750b27e16df897f3d00dc6bef2997ae0"
    },
    {
      "sequencer_type": "cdk-erigon",
      "consensus_type": "cdk-validium",
      "deployment_suffix": "-002",
      "l2_chain_id": 20202,
      "network_id": 2,
      // ... same address fields with unique values
    }
    // ... more networks
  ]
}
```

## Future Enhancements (Out of Scope)

- Support for additional sequencer/consensus combinations
- Automated testing in CI/CD
- Snapshot compression and optimization
- Support for restoring from snapshot back to Kurtosis
- Incremental snapshots (only capture changes)
- L2 state extraction (not needed - services start fresh)
- Support for additional services (blockscout, grafana, etc.)
